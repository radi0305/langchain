#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‘œì¤€ í•œì˜í•™ ìš©ì–´ì§‘ ê´€ë¦¬ì - medical_terms_manager_improved.py (ê°œì„ ëœ ë²„ì „)
í•˜ë“œì½”ë”©ëœ ê¸°ë³¸ ë§¤í•‘ì„ ê°•í™”í•˜ê³  ê´€ê³„ ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ ì‹œìŠ¤í…œ êµ¬ì¶•
ëŒ€í•œí•œì˜í•™íšŒ í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê¸°ë°˜ ì§€ëŠ¥í˜• ì¿¼ë¦¬ í™•ì¥ ì‹œìŠ¤í…œ
"""

import json
import pickle
import networkx as nx
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings("ignore")


class MedicalTermsManager:
    """í‘œì¤€ í•œì˜í•™ ìš©ì–´ì§‘ ê¸°ë°˜ ê´€ë¦¬ì (ê°œì„ ëœ ë²„ì „)"""

    def __init__(self,
                 terms_file: str = "/Users/radi/Projects/langchain/hmedicalterms.json",
                 cache_path: str = "/Users/radi/Projects/langchainDATA/RAWDATA/DYBG/cache"):
        """í‘œì¤€ìš©ì–´ì§‘ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.terms_file = Path(terms_file)
        self.cache_path = Path(cache_path)
        self.cache_path.mkdir(parents=True, exist_ok=True)

        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        self.cache_file = self.cache_path / 'medical_terms_index.pkl'

        # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.terms_data = []
        self.search_index = {}
        self.category_index = {}
        self.synonym_index = {}
        self.hierarchical_index = {}

        # ê°œì„ ëœ ê¸°ëŠ¥ë“¤
        self.relationship_graph = nx.Graph()
        self.semantic_clusters = {}
        self.co_occurrence_matrix = defaultdict(lambda: defaultdict(int))
        self.expansion_patterns = {}
        self.domain_knowledge_base = {}

        # ë¡œë”© ì‹œë„
        self._load_or_build_index()

        # ê³ ê¸‰ ë¶„ì„ ìˆ˜í–‰
        self._build_relationship_graph()
        self._analyze_semantic_clusters()
        self._build_domain_knowledge_base()

    def _load_or_build_index(self):
        """ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            if self._should_rebuild_cache():
                print("ğŸ“š í‘œì¤€ìš©ì–´ì§‘ ê³ ê¸‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                self._build_index()
                self._save_cache()
                print("âœ… í‘œì¤€ìš©ì–´ì§‘ ê³ ê¸‰ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            else:
                print("ğŸ“š í‘œì¤€ìš©ì–´ì§‘ ìºì‹œì—ì„œ ë¡œë”© ì¤‘...")
                self._load_cache()
                print("âœ… í‘œì¤€ìš©ì–´ì§‘ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            print("ğŸ“š ê³ ê¸‰ ê¸°ë³¸ ì¸ë±ìŠ¤ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self._initialize_advanced_basic_index()

    def _should_rebuild_cache(self) -> bool:
        """ìºì‹œ ì¬ìƒì„± í•„ìš” ì—¬ë¶€ í™•ì¸"""
        if not self.cache_file.exists():
            return True

        if not self.terms_file.exists():
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.terms_file}")
            return False

        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ë¹„êµ
        try:
            cache_mtime = self.cache_file.stat().st_mtime
            terms_mtime = self.terms_file.stat().st_mtime
            return terms_mtime > cache_mtime
        except Exception:
            return True

    def _build_index(self):
        """í‘œì¤€ìš©ì–´ì§‘ ì¸ë±ìŠ¤ ìƒì„±"""
        if not self.terms_file.exists():
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.terms_file}")
            self._initialize_advanced_basic_index()
            return

        try:
            with open(self.terms_file, 'r', encoding='utf-8') as f:
                self.terms_data = json.load(f)

            print(f"ğŸ“Š {len(self.terms_data)}ê°œ ìš©ì–´ ë¡œë“œ ì™„ë£Œ")

            # ê°ì¢… ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_search_index()
            self._build_category_index()
            self._build_synonym_index()
            self._build_hierarchical_index()

            # ì¶”ê°€ ì¸ë±ìŠ¤ êµ¬ì¶•
            self._build_co_occurrence_matrix()
            self._extract_expansion_patterns()

        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self._initialize_advanced_basic_index()

    def _build_search_index(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.search_index = {}

        for term_data in self.terms_data:
            # ê¸°ë³¸ ìš©ì–´ëª…
            term_name = term_data.get('ìš©ì–´ëª…', '')
            if term_name:
                self.search_index[term_name] = term_data

            # í•œìëª…
            hanja_name = term_data.get('ìš©ì–´ëª…_í•œì', '')
            if hanja_name and hanja_name != term_name:
                self.search_index[hanja_name] = term_data

            # ë™ì˜ì–´
            synonyms = term_data.get('ë™ì˜ì–´', [])
            for synonym in synonyms:
                if synonym:
                    self.search_index[synonym] = term_data

            # ê²€ìƒ‰í‚¤ì›Œë“œ
            keywords = term_data.get('ê²€ìƒ‰í‚¤ì›Œë“œ', [])
            for keyword in keywords:
                if keyword and keyword not in self.search_index:
                    self.search_index[keyword] = term_data

    def _build_category_index(self):
        """ë¶„ë¥˜ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.category_index = defaultdict(list)

        for term_data in self.terms_data:
            category = term_data.get('ë¶„ë¥˜', 'ê¸°íƒ€')
            self.category_index[category].append(term_data)

    def _build_synonym_index(self):
        """ë™ì˜ì–´ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.synonym_index = {}

        for term_data in self.terms_data:
            term_name = term_data.get('ìš©ì–´ëª…', '')
            synonyms = term_data.get('ë™ì˜ì–´', [])

            if term_name and synonyms:
                all_terms = [term_name] + synonyms
                # ê° ìš©ì–´ì— ëŒ€í•´ ë‹¤ë¥¸ ëª¨ë“  ìš©ì–´ë¥¼ ë™ì˜ì–´ë¡œ ë“±ë¡
                for term in all_terms:
                    if term:
                        related = [t for t in all_terms if t != term]
                        self.synonym_index[term] = related

    def _build_hierarchical_index(self):
        """ê³„ì¸µêµ¬ì¡° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.hierarchical_index = {}

        for term_data in self.terms_data:
            term_name = term_data.get('ìš©ì–´ëª…', '')
            hierarchy = term_data.get('ê³„ì¸µêµ¬ì¡°', {})

            if term_name and hierarchy:
                self.hierarchical_index[term_name] = {
                    'parents': hierarchy.get('ìƒìœ„ê°œë…', []),
                    'children': hierarchy.get('í•˜ìœ„ê°œë…', [])
                }

    def _build_co_occurrence_matrix(self):
        """ê³µê¸° í–‰ë ¬ êµ¬ì¶• (ìš©ì–´ ê°„ ê´€ë ¨ì„± ë¶„ì„)"""
        print("ğŸ”— ìš©ì–´ ê°„ ê³µê¸° ê´€ê³„ ë¶„ì„ ì¤‘...")

        for term_data in self.terms_data:
            term_name = term_data.get('ìš©ì–´ëª…', '')
            category = term_data.get('ë¶„ë¥˜', '')
            keywords = term_data.get('ê²€ìƒ‰í‚¤ì›Œë“œ', [])
            hierarchy = term_data.get('ê³„ì¸µêµ¬ì¡°', {})

            # ê°™ì€ ì¹´í…Œê³ ë¦¬ ìš©ì–´ë“¤ ê°„ì˜ ê´€ë ¨ì„±
            if category:
                category_terms = [t.get('ìš©ì–´ëª…', '')
                                  for t in self.category_index[category]]
                for related_term in category_terms:
                    if related_term and related_term != term_name:
                        self.co_occurrence_matrix[term_name][related_term] += 1

            # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„±
            for keyword in keywords:
                if keyword and keyword != term_name:
                    self.co_occurrence_matrix[term_name][keyword] += 2

            # ê³„ì¸µêµ¬ì¡° ê¸°ë°˜ ê´€ë ¨ì„±
            parents = hierarchy.get('ìƒìœ„ê°œë…', [])
            children = hierarchy.get('í•˜ìœ„ê°œë…', [])

            for parent in parents:
                if parent:
                    self.co_occurrence_matrix[term_name][parent] += 3

            for child in children:
                if child:
                    self.co_occurrence_matrix[term_name][child] += 3

    def _extract_expansion_patterns(self):
        """í™•ì¥ íŒ¨í„´ ì¶”ì¶œ"""
        print("ğŸ” í™•ì¥ íŒ¨í„´ ë¶„ì„ ì¤‘...")

        # ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ ë¶„ì„
        category_patterns = defaultdict(list)

        for category, terms in self.category_index.items():
            if len(terms) < 5:  # ìµœì†Œ 5ê°œ ì´ìƒì˜ ìš©ì–´ê°€ ìˆëŠ” ì¹´í…Œê³ ë¦¬ë§Œ
                continue

            # ê³µí†µ íŒ¨í„´ ì¶”ì¶œ
            common_chars = self._find_common_patterns(
                [t.get('ìš©ì–´ëª…_í•œì', '') for t in terms])
            if common_chars:
                category_patterns[category] = common_chars

        self.expansion_patterns = dict(category_patterns)

    def _find_common_patterns(self, terms: List[str]) -> List[str]:
        """ê³µí†µ íŒ¨í„´ ì°¾ê¸°"""
        if not terms or len(terms) < 3:
            return []

        patterns = []

        # ê³µí†µ ì ‘ë¯¸ì‚¬ ì°¾ê¸°
        suffixes = defaultdict(int)
        for term in terms:
            if len(term) >= 2:
                for i in range(1, min(4, len(term) + 1)):  # ìµœëŒ€ 3ê¸€ìê¹Œì§€
                    suffix = term[-i:]
                    suffixes[suffix] += 1

        # 30% ì´ìƒì˜ ìš©ì–´ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ë§Œ ì„ íƒ
        threshold = len(terms) * 0.3
        for suffix, count in suffixes.items():
            if count >= threshold and len(suffix) >= 2:
                patterns.append(suffix)

        return patterns[:5]  # ìƒìœ„ 5ê°œë§Œ

    def _build_relationship_graph(self):
        """ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶•"""
        print("ğŸ•¸ï¸ ìš©ì–´ ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶• ì¤‘...")

        self.relationship_graph = nx.Graph()

        for term_data in self.terms_data:
            term_name = term_data.get('ìš©ì–´ëª…', '')
            if not term_name:
                continue

            self.relationship_graph.add_node(term_name, **term_data)

            # ë™ì˜ì–´ ê´€ê³„
            synonyms = term_data.get('ë™ì˜ì–´', [])
            for synonym in synonyms:
                if synonym:
                    self.relationship_graph.add_edge(term_name, synonym,
                                                     relation='synonym', weight=1.0)

            # ê³„ì¸µêµ¬ì¡° ê´€ê³„
            hierarchy = term_data.get('ê³„ì¸µêµ¬ì¡°', {})
            parents = hierarchy.get('ìƒìœ„ê°œë…', [])
            children = hierarchy.get('í•˜ìœ„ê°œë…', [])

            for parent in parents:
                if parent:
                    self.relationship_graph.add_edge(term_name, parent,
                                                     relation='parent', weight=0.8)

            for child in children:
                if child:
                    self.relationship_graph.add_edge(term_name, child,
                                                     relation='child', weight=0.8)

            # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê´€ê³„
            category = term_data.get('ë¶„ë¥˜', '')
            if category:
                category_terms = [t.get('ìš©ì–´ëª…', '')
                                  for t in self.category_index[category]]
                for related_term in category_terms[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ
                    if related_term and related_term != term_name:
                        if not self.relationship_graph.has_edge(term_name, related_term):
                            self.relationship_graph.add_edge(term_name, related_term,
                                                             relation='category', weight=0.3)

        print(f"âœ… ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ: {self.relationship_graph.number_of_nodes()}ê°œ ë…¸ë“œ, "
              f"{self.relationship_graph.number_of_edges()}ê°œ ì—£ì§€")

    def _analyze_semantic_clusters(self):
        """ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
        print("ğŸ”¬ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")

        try:
            # ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
            communities = nx.community.greedy_modularity_communities(
                self.relationship_graph)

            cluster_id = 0
            for community in communities:
                if len(community) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì˜ ìš©ì–´ë¡œ êµ¬ì„±ëœ í´ëŸ¬ìŠ¤í„°ë§Œ
                    cluster_name = f"cluster_{cluster_id}"
                    self.semantic_clusters[cluster_name] = {
                        'terms': list(community),
                        'size': len(community),
                        'dominant_category': self._get_dominant_category(community)
                    }
                    cluster_id += 1

            print(f"âœ… {len(self.semantic_clusters)}ê°œ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë°œê²¬")

        except Exception as e:
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.semantic_clusters = {}

    def _get_dominant_category(self, terms: Set[str]) -> str:
        """í´ëŸ¬ìŠ¤í„°ì˜ ì§€ë°°ì  ì¹´í…Œê³ ë¦¬ ì°¾ê¸°"""
        category_counts = Counter()

        for term in terms:
            term_data = self.search_index.get(term)
            if term_data:
                category = term_data.get('ë¶„ë¥˜', 'ê¸°íƒ€')
                category_counts[category] += 1

        if category_counts:
            return category_counts.most_common(1)[0][0]
        return 'ê¸°íƒ€'

    def _build_domain_knowledge_base(self):
        """ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• (ê°œì„ ëœ ë§¤í•‘)"""
        print("ğŸ§  ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")

        self.domain_knowledge_base = {
            # í—ˆì¦ ê´€ë ¨ ê³ ê¸‰ ë§¤í•‘
            'deficiency_patterns': {
                'è¡€è™›': {
                    'primary_herbs': ['ç•¶æ­¸', 'ç†Ÿåœ°é»ƒ', 'ç™½èŠ', 'å·èŠ'],
                    'primary_prescriptions': ['å››ç‰©æ¹¯', 'ç•¶æ­¸è£œè¡€æ¹¯', 'å…«çæ¹¯'],
                    'related_symptoms': ['é¢è‰²èé»ƒ', 'å¿ƒæ‚¸', 'å¤±çœ ', 'æœˆç¶“ä¸èª¿'],
                    'treatment_principles': ['è£œè¡€', 'é¤Šè¡€', 'èª¿ç¶“'],
                    'related_theories': ['ç‡Ÿè¡€ä¸è¶³', 'å¿ƒè¡€è™›', 'è‚è¡€è™›']
                },
                'æ°£è™›': {
                    'primary_herbs': ['äººåƒ', 'é»ƒèŠª', 'ç™½æœ®', 'ç”˜è‰'],
                    'primary_prescriptions': ['å››å›å­æ¹¯', 'è£œä¸­ç›Šæ°£æ¹¯', 'åƒè‹“ç™½æœ®æ•£'],
                    'related_symptoms': ['ç¥ç–²ä¹åŠ›', 'æ°£çŸ­', 'è‡ªæ±—', 'è„«è‚›'],
                    'treatment_principles': ['è£œæ°£', 'ç›Šæ°£', 'å‡é™½'],
                    'related_theories': ['ä¸­æ°£ä¸è¶³', 'è„¾æ°£è™›', 'è‚ºæ°£è™›']
                },
                'é™°è™›': {
                    'primary_herbs': ['ç”Ÿåœ°é»ƒ', 'éº¥é–€å†¬', 'ç„åƒ', 'çŸ³æ–›'],
                    'primary_prescriptions': ['å…­å‘³åœ°é»ƒæ¹¯', 'éº¥å‘³åœ°é»ƒæ¹¯', 'çŸ¥æŸåœ°é»ƒæ¹¯'],
                    'related_symptoms': ['æ½®ç†±', 'ç›œæ±—', 'äº”å¿ƒç…©ç†±', 'å£ç‡¥å’½ä¹¾'],
                    'treatment_principles': ['æ»‹é™°', 'é¤Šé™°', 'æ¸…ç†±'],
                    'related_theories': ['è…é™°è™›', 'è‚ºé™°è™›', 'èƒƒé™°è™›']
                },
                'é™½è™›': {
                    'primary_herbs': ['é™„å­', 'è‚‰æ¡‚', 'ä¹¾è–‘', 'é¹¿èŒ¸'],
                    'primary_prescriptions': ['è…æ°£ä¸¸', 'å³æ­¸ä¸¸', 'ç†ä¸­æ¹¯'],
                    'related_symptoms': ['ç•å¯’è‚¢å†·', 'è…°è†é…¸è»Ÿ', 'é™½è', 'ä¹…ç€‰'],
                    'treatment_principles': ['æº«é™½', 'åŠ©é™½', 'è£œé™½'],
                    'related_theories': ['è…é™½è™›', 'è„¾é™½è™›', 'å¿ƒé™½è™›']
                }
            },

            # ì²˜ë°© ê´€ê³„ ë§¤í•‘
            'prescription_relationships': {
                'å››ç‰©æ¹¯': {
                    'base_formula': True,
                    'derived_formulas': ['å…«ç‰©æ¹¯', 'åå…¨å¤§è£œæ¹¯', 'è† è‰¾æ¹¯'],
                    'combination_formulas': ['é€é™æ•£', 'æº«ç¶“æ¹¯'],
                    'modification_principles': ['åŠ æ¸›æ³•', 'åˆæ–¹æ³•']
                },
                'å››å›å­æ¹¯': {
                    'base_formula': True,
                    'derived_formulas': ['å…­å›å­æ¹¯', 'é¦™ç ‚å…­å›å­æ¹¯', 'åƒè‹“ç™½æœ®æ•£'],
                    'combination_formulas': ['å…«çæ¹¯', 'åå…¨å¤§è£œæ¹¯'],
                    'modification_principles': ['ç†æ°£', 'åŒ–ç—°', 'å¥è„¾']
                }
            },

            # ì•½ë¬¼ ê´€ê³„ ë§¤í•‘
            'herb_relationships': {
                'äººåƒ': {
                    'similar_herbs': ['é»¨åƒ', 'è¥¿æ´‹åƒ', 'å¤ªå­åƒ'],
                    'synergistic_herbs': ['é»ƒèŠª', 'ç™½æœ®', 'èŒ¯è‹“'],
                    'antagonistic_herbs': ['èŠè”å­', 'äº”éˆè„‚'],
                    'processing_methods': ['ç”Ÿæ›¬åƒ', 'ç´…åƒ', 'åƒé¬š']
                },
                'ç•¶æ­¸': {
                    'similar_herbs': ['å·èŠ', 'ç™½èŠ', 'ç†Ÿåœ°é»ƒ'],
                    'synergistic_herbs': ['å·èŠ', 'é»ƒèŠª', 'ç´…èŠ±'],
                    'part_usage': ['ç•¶æ­¸é ­', 'ç•¶æ­¸èº«', 'ç•¶æ­¸å°¾'],
                    'processing_methods': ['é…’ç•¶æ­¸', 'åœŸç‚’ç•¶æ­¸']
                }
            },

            # ì´ë¡  ì²´ê³„ ë§¤í•‘
            'theoretical_frameworks': {
                'é™°é™½': {
                    'related_concepts': ['é™°é™½å¹³è¡¡', 'é™°é™½äº’æ ¹', 'é™°é™½è½‰åŒ–'],
                    'clinical_applications': ['å¯’ç†±è¾¨è­‰', 'è™›å¯¦è¾¨è­‰', 'è¡¨è£è¾¨è­‰'],
                    'related_theories': ['äº”è¡Œ', 'è‡Ÿè…‘', 'ç¶“çµ¡']
                },
                'äº”è¡Œ': {
                    'related_concepts': ['äº”è¡Œç›¸ç”Ÿ', 'äº”è¡Œç›¸å…‹', 'äº”è¡Œåˆ¶åŒ–'],
                    'clinical_applications': ['äº”è‡Ÿè¾¨è­‰', 'æƒ…å¿—èª¿æ”', 'äº”å‘³èª¿é¤Š'],
                    'related_theories': ['è‡Ÿè…‘', 'ç¶“çµ¡', 'ç—…æ©Ÿ']
                }
            }
        }

    def _save_cache(self):
        """ìºì‹œ ì €ì¥ (ê°œì„ ëœ ë°ì´í„° í¬í•¨)"""
        try:
            cache_data = {
                'terms_data': self.terms_data,
                'search_index': self.search_index,
                'category_index': dict(self.category_index),
                'synonym_index': self.synonym_index,
                'hierarchical_index': self.hierarchical_index,
                'co_occurrence_matrix': dict(self.co_occurrence_matrix),
                'expansion_patterns': self.expansion_patterns,
                'semantic_clusters': self.semantic_clusters,
                'domain_knowledge_base': self.domain_knowledge_base,
                'relationship_graph_data': nx.node_link_data(self.relationship_graph),
                'timestamp': self._get_current_timestamp()
            }

            with open(self.cache_file, 'wb') as f:
                pickle.dump(cache_data, f)

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ (ê°œì„ ëœ ë°ì´í„° í¬í•¨)"""
        try:
            with open(self.cache_file, 'rb') as f:
                cache_data = pickle.load(f)

            self.terms_data = cache_data.get('terms_data', [])
            self.search_index = cache_data.get('search_index', {})
            self.category_index = cache_data.get('category_index', {})
            self.synonym_index = cache_data.get('synonym_index', {})
            self.hierarchical_index = cache_data.get('hierarchical_index', {})
            self.co_occurrence_matrix = cache_data.get(
                'co_occurrence_matrix', {})
            self.expansion_patterns = cache_data.get('expansion_patterns', {})
            self.semantic_clusters = cache_data.get('semantic_clusters', {})
            self.domain_knowledge_base = cache_data.get(
                'domain_knowledge_base', {})

            # ê´€ê³„ ê·¸ë˜í”„ ë³µì›
            graph_data = cache_data.get('relationship_graph_data')
            if graph_data:
                self.relationship_graph = nx.node_link_graph(graph_data)
            else:
                self.relationship_graph = nx.Graph()

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._initialize_advanced_basic_index()

    def _initialize_advanced_basic_index(self):
        """ê³ ê¸‰ ê¸°ë³¸ ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ê°œì„ ëœ í´ë°±)"""
        print("ğŸ“š ê³ ê¸‰ ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")

        # ê¸°ë³¸ ìš©ì–´ ë°ì´í„° (ë” í’ë¶€í•œ ì •ë³´ í¬í•¨)
        basic_terms = [
            {
                'ìš©ì–´ëª…': 'í˜ˆí—ˆ', 'ìš©ì–´ëª…_í•œì': 'è¡€è™›', 'ë¶„ë¥˜': 'ë³‘ì¦',
                'ë™ì˜ì–´': ['í˜ˆë¶€ì¡±', 'í˜ˆì•¡ë¶€ì¡±'],
                'ê²€ìƒ‰í‚¤ì›Œë“œ': ['í˜ˆí—ˆ', 'í˜ˆë¶€ì¡±', 'è¡€è™›', 'í˜ˆì•¡ë¶€ì¡±'],
                'ê³„ì¸µêµ¬ì¡°': {
                    'ìƒìœ„ê°œë…': ['í—ˆì¦', 'í˜ˆë³‘'],
                    'í•˜ìœ„ê°œë…': ['ì‹¬í˜ˆí—ˆ', 'ê°„í˜ˆí—ˆ']
                }
            },
            {
                'ìš©ì–´ëª…': 'ê¸°í—ˆ', 'ìš©ì–´ëª…_í•œì': 'æ°£è™›', 'ë¶„ë¥˜': 'ë³‘ì¦',
                'ë™ì˜ì–´': ['ê¸°ë¶€ì¡±'],
                'ê²€ìƒ‰í‚¤ì›Œë“œ': ['ê¸°í—ˆ', 'ê¸°ë¶€ì¡±', 'æ°£è™›'],
                'ê³„ì¸µêµ¬ì¡°': {
                    'ìƒìœ„ê°œë…': ['í—ˆì¦', 'ê¸°ë³‘'],
                    'í•˜ìœ„ê°œë…': ['íê¸°í—ˆ', 'ë¹„ê¸°í—ˆ']
                }
            },
            {
                'ìš©ì–´ëª…': 'ì‚¬ë¬¼íƒ•', 'ìš©ì–´ëª…_í•œì': 'å››ç‰©æ¹¯', 'ë¶„ë¥˜': 'ì²˜ë°©',
                'ë™ì˜ì–´': [],
                'ê²€ìƒ‰í‚¤ì›Œë“œ': ['ì‚¬ë¬¼íƒ•', 'å››ç‰©æ¹¯', 'ë³´í˜ˆì œ'],
                'ê³„ì¸µêµ¬ì¡°': {
                    'ìƒìœ„ê°œë…': ['ë³´í˜ˆì œ', 'ë°©ì œ'],
                    'í•˜ìœ„ê°œë…': ['ê°€ê°ì‚¬ë¬¼íƒ•']
                }
            },
            {
                'ìš©ì–´ëª…': 'ì¸ì‚¼', 'ìš©ì–´ëª…_í•œì': 'äººåƒ', 'ë¶„ë¥˜': 'ì•½ë¬¼',
                'ë™ì˜ì–´': ['ê³ ë ¤ì‚¼', 'í™ì‚¼'],
                'ê²€ìƒ‰í‚¤ì›Œë“œ': ['ì¸ì‚¼', 'äººåƒ', 'ê³ ë ¤ì‚¼', 'í™ì‚¼', 'ëŒ€ë³´ì›ê¸°'],
                'ê³„ì¸µêµ¬ì¡°': {
                    'ìƒìœ„ê°œë…': ['ë³´ê¸°ì•½', 'ë³¸ì´ˆ'],
                    'í•˜ìœ„ê°œë…': ['ìƒì§„ì‚¼', 'í™ì‚¼']
                }
            }
        ]

        self.terms_data = basic_terms
        self._build_search_index()
        self._build_category_index()
        self._build_synonym_index()
        self._build_hierarchical_index()

        # ê¸°ë³¸ ë„ë©”ì¸ ì§€ì‹ êµ¬ì¶•
        self._build_basic_domain_knowledge()

        # ê¸°ë³¸ ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶•
        self.relationship_graph = nx.Graph()
        for term_data in basic_terms:
            term_name = term_data['ìš©ì–´ëª…']
            self.relationship_graph.add_node(term_name, **term_data)

    def _build_basic_domain_knowledge(self):
        """ê¸°ë³¸ ë„ë©”ì¸ ì§€ì‹ êµ¬ì¶•"""
        self.domain_knowledge_base = {
            'deficiency_patterns': {
                'è¡€è™›': {
                    'primary_herbs': ['ç•¶æ­¸', 'ç†Ÿåœ°é»ƒ', 'ç™½èŠ', 'å·èŠ'],
                    'primary_prescriptions': ['å››ç‰©æ¹¯', 'ç•¶æ­¸è£œè¡€æ¹¯'],
                    'related_symptoms': ['ë©´ìƒ‰ìœ„í™©', 'ì‹¬ê³„', 'ì‹¤ë©´'],
                    'treatment_principles': ['è£œè¡€', 'é¤Šè¡€']
                },
                'æ°£è™›': {
                    'primary_herbs': ['äººåƒ', 'é»ƒèŠª', 'ç™½æœ®', 'ç”˜è‰'],
                    'primary_prescriptions': ['å››å›å­æ¹¯', 'è£œä¸­ç›Šæ°£æ¹¯'],
                    'related_symptoms': ['ì‹ í”¼í•ë ¥', 'ê¸°ë‹¨', 'ìí•œ'],
                    'treatment_principles': ['è£œæ°£', 'ç›Šæ°£']
                }
            }
        }

    def _get_current_timestamp(self):
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        return datetime.now().isoformat()

    def expand_query(self, query: str, max_expansions: int = 10) -> List[str]:
        """
        ê³ ê¸‰ ì§€ëŠ¥í˜• ì¿¼ë¦¬ í™•ì¥ (ê°œì„ ëœ ë²„ì „)
        ë‹¤ì¸µ í™•ì¥ ì „ëµ: ì§ì ‘ë§¤ì¹­ â†’ ê´€ê³„ê·¸ë˜í”„ â†’ ë„ë©”ì¸ì§€ì‹ â†’ ì˜ë¯¸í´ëŸ¬ìŠ¤í„° â†’ ê³µê¸°ê´€ê³„ â†’ íŒ¨í„´ë§¤ì¹­
        """
        expansions = set([query])  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©

        try:
            # 1ë‹¨ê³„: ì§ì ‘ ë§¤ì¹­ ë° ê¸°ë³¸ í™•ì¥
            if query in self.search_index:
                basic_expansions = self._get_basic_expansions(query)
                expansions.update(basic_expansions)

            # 2ë‹¨ê³„: ê´€ê³„ ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥
            if self.relationship_graph.has_node(query):
                graph_expansions = self._get_graph_based_expansions(query)
                expansions.update(graph_expansions)

            # 3ë‹¨ê³„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í™•ì¥
            domain_expansions = self._get_domain_knowledge_expansions(query)
            expansions.update(domain_expansions)

            # 4ë‹¨ê³„: ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í™•ì¥
            cluster_expansions = self._get_cluster_based_expansions(query)
            expansions.update(cluster_expansions)

            # 5ë‹¨ê³„: ê³µê¸° ê´€ê³„ ê¸°ë°˜ í™•ì¥
            co_occurrence_expansions = self._get_co_occurrence_expansions(
                query)
            expansions.update(co_occurrence_expansions)

            # 6ë‹¨ê³„: íŒ¨í„´ ê¸°ë°˜ í™•ì¥
            pattern_expansions = self._get_pattern_based_expansions(query)
            expansions.update(pattern_expansions)

            # 7ë‹¨ê³„: ì§€ëŠ¥ì  ë¶„í•  ë° ì¡°í•©
            split_expansions = self.split_query_intelligently(query)
            expansions.update(split_expansions)

            # í’ˆì§ˆ í•„í„°ë§ ë° ìˆœìœ„ ë§¤ê¸°ê¸°
            filtered_expansions = self._filter_and_rank_expansions(
                query, list(expansions))

            return filtered_expansions[:max_expansions]

        except Exception as e:
            print(f"âš ï¸ ê³ ê¸‰ ì¿¼ë¦¬ í™•ì¥ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ í™•ì¥
            return self._get_basic_expansions(query)[:max_expansions]

    def _get_basic_expansions(self, query: str) -> List[str]:
        """ê¸°ë³¸ í™•ì¥ (1ë‹¨ê³„)"""
        expansions = []

        if query in self.search_index:
            term_data = self.search_index[query]

            # ë™ì˜ì–´ ì¶”ê°€
            synonyms = term_data.get('ë™ì˜ì–´', [])
            expansions.extend(synonyms[:3])

            # í•œì/í•œê¸€ëª… ìƒí˜¸ ì¶”ê°€
            hanja = term_data.get('ìš©ì–´ëª…_í•œì', '')
            hangul = term_data.get('ìš©ì–´ëª…', '')

            if hanja and hanja != query:
                expansions.append(hanja)
            if hangul and hangul != query:
                expansions.append(hangul)

            # ê²€ìƒ‰í‚¤ì›Œë“œ ì¶”ê°€
            keywords = term_data.get('ê²€ìƒ‰í‚¤ì›Œë“œ', [])
            expansions.extend([k for k in keywords[:3] if k != query])

        return expansions

    def _get_graph_based_expansions(self, query: str) -> List[str]:
        """ê´€ê³„ ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ (2ë‹¨ê³„)"""
        expansions = []
        try:
            # ì§ì ‘ ì´ì›ƒ ë…¸ë“œë“¤
            neighbors = list(self.relationship_graph.neighbors(query))

            # ê´€ê³„ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            weighted_neighbors = []
            for neighbor in neighbors:
                edge_data = self.relationship_graph.get_edge_data(
                    query, neighbor)
                relation = edge_data.get('relation', 'unknown')
                weight = edge_data.get('weight', 0.5)

                # ê´€ê³„ë³„ ìš°ì„ ìˆœìœ„
                priority_map = {
                    'synonym': 1.0,
                    'parent': 0.8,
                    'child': 0.8,
                    'category': 0.3
                }

                final_weight = weight * priority_map.get(relation, 0.1)
                weighted_neighbors.append((neighbor, final_weight))

            # ê°€ì¤‘ì¹˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ í•­ëª© ì„ íƒ
            weighted_neighbors.sort(key=lambda x: x[1], reverse=True)
            expansions.extend(
                [neighbor for neighbor, _ in weighted_neighbors[:5]])

            # 2í™‰ ì´ì›ƒë„ ê³ ë ¤ (ê°€ì¤‘ì¹˜ ê°ì†Œ)
            if len(expansions) < 3:
                two_hop_neighbors = []
                for neighbor in neighbors[:3]:
                    second_neighbors = list(
                        self.relationship_graph.neighbors(neighbor))
                    two_hop_neighbors.extend(
                        [n for n in second_neighbors if n != query])

                expansions.extend(two_hop_neighbors[:2])

        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ ì‹¤íŒ¨: {e}")

        return expansions

    def _get_domain_knowledge_expansions(self, query: str) -> List[str]:
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í™•ì¥ (3ë‹¨ê³„)"""
        expansions = []

        try:
            # í—ˆì¦ íŒ¨í„´ ë§¤ì¹­
            deficiency_patterns = self.domain_knowledge_base.get(
                'deficiency_patterns', {})
            if query in deficiency_patterns:
                pattern_data = deficiency_patterns[query]
                expansions.extend(pattern_data.get('primary_herbs', [])[:2])
                expansions.extend(pattern_data.get(
                    'primary_prescriptions', [])[:2])
                expansions.extend(pattern_data.get('related_theories', [])[:1])

            # ì²˜ë°© ê´€ê³„ ë§¤í•‘
            prescription_relationships = self.domain_knowledge_base.get(
                'prescription_relationships', {})
            if query in prescription_relationships:
                relationship_data = prescription_relationships[query]
                expansions.extend(relationship_data.get(
                    'derived_formulas', [])[:2])
                expansions.extend(relationship_data.get(
                    'combination_formulas', [])[:1])

            # ì•½ë¬¼ ê´€ê³„ ë§¤í•‘
            herb_relationships = self.domain_knowledge_base.get(
                'herb_relationships', {})
            if query in herb_relationships:
                herb_data = herb_relationships[query]
                expansions.extend(herb_data.get('similar_herbs', [])[:2])
                expansions.extend(herb_data.get('synergistic_herbs', [])[:1])

            # ì´ë¡  ì²´ê³„ ë§¤í•‘
            theoretical_frameworks = self.domain_knowledge_base.get(
                'theoretical_frameworks', {})
            if query in theoretical_frameworks:
                theory_data = theoretical_frameworks[query]
                expansions.extend(theory_data.get('related_concepts', [])[:2])

        except Exception as e:
            print(f"âš ï¸ ë„ë©”ì¸ ì§€ì‹ í™•ì¥ ì‹¤íŒ¨: {e}")

        return expansions

    def _get_cluster_based_expansions(self, query: str) -> List[str]:
        """ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í™•ì¥ (4ë‹¨ê³„)"""
        expansions = []

        try:
            # ì¿¼ë¦¬ê°€ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
            query_clusters = []
            for cluster_name, cluster_data in self.semantic_clusters.items():
                if query in cluster_data['terms']:
                    query_clusters.append(cluster_data)

            # ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë‹¤ë¥¸ ìš©ì–´ë“¤ ì¶”ê°€
            for cluster_data in query_clusters:
                cluster_terms = cluster_data['terms']
                related_terms = [
                    term for term in cluster_terms if term != query]
                expansions.extend(related_terms[:3])  # ìµœëŒ€ 3ê°œê¹Œì§€

        except Exception as e:
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í™•ì¥ ì‹¤íŒ¨: {e}")

        return expansions

    def _get_co_occurrence_expansions(self, query: str) -> List[str]:
        """ê³µê¸° ê´€ê³„ ê¸°ë°˜ í™•ì¥ (5ë‹¨ê³„)"""
        expansions = []

        try:
            if query in self.co_occurrence_matrix:
                co_occurrences = self.co_occurrence_matrix[query]
                # ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_co_occurrences = sorted(co_occurrences.items(),
                                               key=lambda x: x[1], reverse=True)
                expansions.extend(
                    [term for term, _ in sorted_co_occurrences[:3]])

        except Exception as e:
            print(f"âš ï¸ ê³µê¸° ê´€ê³„ í™•ì¥ ì‹¤íŒ¨: {e}")

        return expansions

    def _get_pattern_based_expansions(self, query: str) -> List[str]:
        """íŒ¨í„´ ê¸°ë°˜ í™•ì¥ (6ë‹¨ê³„)"""
        expansions = []

        try:
            # ì¿¼ë¦¬ì˜ ì¹´í…Œê³ ë¦¬ í™•ì¸
            term_data = self.search_index.get(query)
            if term_data:
                category = term_data.get('ë¶„ë¥˜', '')
                if category in self.expansion_patterns:
                    patterns = self.expansion_patterns[category]

                    # ê°™ì€ íŒ¨í„´ì„ ê°€ì§„ ë‹¤ë¥¸ ìš©ì–´ë“¤ ì°¾ê¸°
                    for pattern in patterns:
                        if pattern in query:
                            # ê°™ì€ íŒ¨í„´ì˜ ë‹¤ë¥¸ ìš©ì–´ë“¤ ê²€ìƒ‰
                            pattern_terms = [term for term in self.search_index.keys()
                                             if pattern in term and term != query]
                            expansions.extend(pattern_terms[:2])

        except Exception as e:
            print(f"âš ï¸ íŒ¨í„´ ê¸°ë°˜ í™•ì¥ ì‹¤íŒ¨: {e}")

        return expansions

    def _filter_and_rank_expansions(self, query: str, expansions: List[str]) -> List[str]:
        """í™•ì¥ ê²°ê³¼ í•„í„°ë§ ë° ìˆœìœ„ ë§¤ê¸°ê¸°"""
        if not expansions:
            return []

        scored_expansions = []

        for expansion in expansions:
            if expansion == query:
                continue

            score = 0.0

            # 1. ê¸¸ì´ ìœ ì‚¬ì„± (ë„ˆë¬´ ë‹¤ë¥´ë©´ ê´€ë ¨ì„± ë‚®ìŒ)
            length_diff = abs(len(expansion) - len(query))
            if length_diff <= 2:
                score += 1.0
            elif length_diff <= 4:
                score += 0.5

            # 2. ê¸€ì ê²¹ì¹¨ ì •ë„
            common_chars = set(query) & set(expansion)
            overlap_ratio = len(common_chars) / \
                max(len(set(query)), len(set(expansion)))
            score += overlap_ratio * 2.0

            # 3. ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ì„±
            query_data = self.search_index.get(query)
            expansion_data = self.search_index.get(expansion)

            if query_data and expansion_data:
                query_category = query_data.get('ë¶„ë¥˜', '')
                expansion_category = expansion_data.get('ë¶„ë¥˜', '')

                if query_category == expansion_category:
                    score += 1.5
                elif query_category and expansion_category:
                    # ê´€ë ¨ ì¹´í…Œê³ ë¦¬ì¸ì§€ í™•ì¸
                    related_categories = {
                        'ë³‘ì¦': ['ì¦ìƒ', 'ì§•í›„'],
                        'ì²˜ë°©': ['ì¹˜ë²•'],
                        'ì•½ë¬¼': ['ë³¸ì´ˆ'],
                        'ìƒë¦¬': ['ë³‘ë¦¬', 'ë³€ì¦']
                    }

                    if expansion_category in related_categories.get(query_category, []):
                        score += 0.8

            # 4. ê´€ê³„ ê·¸ë˜í”„ì—ì„œì˜ ê±°ë¦¬
            if (self.relationship_graph.has_node(query) and
                    self.relationship_graph.has_node(expansion)):
                try:
                    distance = nx.shortest_path_length(
                        self.relationship_graph, query, expansion)
                    if distance == 1:
                        score += 2.0
                    elif distance == 2:
                        score += 1.0
                    elif distance <= 3:
                        score += 0.5
                except nx.NetworkXNoPath:
                    pass

            # 5. ì˜ë¯¸ìˆëŠ” ìš©ì–´ì¸ì§€ í™•ì¸
            if self._is_meaningful_expansion(expansion):
                score += 0.5

            scored_expansions.append((expansion, score))

        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_expansions.sort(key=lambda x: x[1], reverse=True)

        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ê²°ê³¼ ë°˜í™˜
        seen = set()
        filtered_result = []

        for expansion, score in scored_expansions:
            if expansion not in seen and score > 0.5:  # ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’
                seen.add(expansion)
                filtered_result.append(expansion)

        return filtered_result

    def _is_meaningful_expansion(self, term: str) -> bool:
        """ì˜ë¯¸ìˆëŠ” í™•ì¥ì¸ì§€ íŒë‹¨"""
        if len(term) < 2:
            return False

        # í•œì ë¹„ìœ¨ í™•ì¸
        chinese_char_count = sum(
            1 for char in term if '\u4e00' <= char <= '\u9fff')
        if chinese_char_count / len(term) < 0.5:
            return False

        # í‘œì¤€ìš©ì–´ì§‘ì— ìˆëŠ”ì§€ í™•ì¸
        return term in self.search_index

    def split_query_intelligently(self, query: str) -> List[str]:
        """ê³ ê¸‰ ì§€ëŠ¥ì  ì¿¼ë¦¬ ë¶„í• """
        parts = [query]

        try:
            # ê¸¸ì´ë³„ ë¶„í•  ì „ëµ (ê°œì„ ë¨)
            if len(query) >= 4:
                # ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  ì‹œë„
                meaningful_parts = self._extract_meaningful_subterms(query)
                parts.extend(meaningful_parts)

                # ê¸¸ì´ë³„ ë¶„í• 
                for i in range(len(query) - 1):
                    if i + 2 <= len(query):
                        part = query[i:i + 2]
                        if self._is_meaningful_expansion(part):
                            parts.append(part)

                for i in range(len(query) - 2):
                    if i + 3 <= len(query):
                        part = query[i:i + 3]
                        if self._is_meaningful_expansion(part):
                            parts.append(part)

            elif len(query) == 3:
                # 3ê¸€ì: ì• 2ê¸€ì, ë’¤ 2ê¸€ì ì¶”ê°€
                front_part = query[:2]
                back_part = query[1:]

                if self._is_meaningful_expansion(front_part):
                    parts.append(front_part)
                if self._is_meaningful_expansion(back_part):
                    parts.append(back_part)

            # ì¤‘ë³µ ì œê±° ë° ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ë§Œ ì„ íƒ
            unique_parts = []
            for part in parts:
                if part not in unique_parts and self._is_meaningful_expansion(part):
                    unique_parts.append(part)

            return unique_parts

        except Exception as e:
            print(f"âš ï¸ ì§€ëŠ¥ì  ë¶„í•  ì‹¤íŒ¨: {e}")
            return [query]

    def _extract_meaningful_subterms(self, query: str) -> List[str]:
        """ì˜ë¯¸ ìˆëŠ” í•˜ìœ„ ìš©ì–´ ì¶”ì¶œ"""
        subterms = []

        # ì•Œë ¤ì§„ íŒ¨í„´ ê¸°ë°˜ ë¶„í• 
        common_suffixes = ['æ¹¯', 'æ•£', 'ä¸¸', 'è†',
                           'è­‰', 'ç—…', 'ç—‡', 'è™›', 'å¯¦', 'ç†±', 'å¯’']

        for suffix in common_suffixes:
            if query.endswith(suffix) and len(query) > len(suffix):
                base_part = query[:-len(suffix)]
                if len(base_part) >= 2 and base_part in self.search_index:
                    subterms.append(base_part)
                if suffix in self.search_index:
                    subterms.append(suffix)

        # ë³µí•©ì–´ ë¶„í•  ì‹œë„
        if len(query) >= 4:
            for i in range(2, len(query)):
                left_part = query[:i]
                right_part = query[i:]

                if (len(left_part) >= 2 and len(right_part) >= 2 and
                        left_part in self.search_index and right_part in self.search_index):
                    subterms.extend([left_part, right_part])

        return subterms

    def get_related_terms(self, query: str, max_terms: int = 10) -> List[str]:
        """ê³ ê¸‰ ê´€ë ¨ ìš©ì–´ ë°˜í™˜"""
        related = []

        try:
            # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê´€ë ¨ ìš©ì–´ ìˆ˜ì§‘
            methods = [
                self._get_basic_related_terms,
                self._get_graph_related_terms,
                self._get_domain_related_terms,
                self._get_cluster_related_terms
            ]

            for method in methods:
                method_results = method(query)
                related.extend(method_results)

            # ì¤‘ë³µ ì œê±° ë° ìˆœìœ„ ë§¤ê¸°ê¸°
            unique_related = []
            seen = set()

            for term in related:
                if term and term != query and term not in seen:
                    seen.add(term)
                    unique_related.append(term)

            return unique_related[:max_terms]

        except Exception as e:
            print(f"âš ï¸ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _get_basic_related_terms(self, query: str) -> List[str]:
        """ê¸°ë³¸ ê´€ë ¨ ìš©ì–´"""
        related = []

        if query in self.search_index:
            term_data = self.search_index[query]

            # ë™ì˜ì–´
            synonyms = term_data.get('ë™ì˜ì–´', [])
            related.extend(synonyms[:3])

            # ê°™ì€ ë¶„ë¥˜ì˜ ë‹¤ë¥¸ ìš©ì–´ë“¤
            category = term_data.get('ë¶„ë¥˜', '')
            if category in self.category_index:
                category_terms = self.category_index[category]
                for cat_term in category_terms[:5]:
                    term_name = cat_term.get('ìš©ì–´ëª…', '')
                    if term_name and term_name != query:
                        related.append(term_name)

        return related

    def _get_graph_related_terms(self, query: str) -> List[str]:
        """ê·¸ë˜í”„ ê¸°ë°˜ ê´€ë ¨ ìš©ì–´"""
        related = []

        if self.relationship_graph.has_node(query):
            neighbors = list(self.relationship_graph.neighbors(query))
            related.extend(neighbors[:5])

        return related

    def _get_domain_related_terms(self, query: str) -> List[str]:
        """ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê´€ë ¨ ìš©ì–´"""
        related = []

        # ë„ë©”ì¸ ì§€ì‹ì—ì„œ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ
        for pattern_type, patterns in self.domain_knowledge_base.items():
            if isinstance(patterns, dict) and query in patterns:
                pattern_data = patterns[query]
                if isinstance(pattern_data, dict):
                    for key, values in pattern_data.items():
                        if isinstance(values, list):
                            related.extend(values[:2])

        return related

    def _get_cluster_related_terms(self, query: str) -> List[str]:
        """í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê´€ë ¨ ìš©ì–´"""
        related = []

        for cluster_data in self.semantic_clusters.values():
            if query in cluster_data['terms']:
                cluster_terms = [
                    term for term in cluster_data['terms'] if term != query]
                related.extend(cluster_terms[:3])

        return related

    def get_term_info(self, term: str) -> Optional[Dict]:
        """íŠ¹ì • ìš©ì–´ì˜ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            if term in self.search_index:
                base_info = self.search_index[term].copy()

                # ì¶”ê°€ ì •ë³´ enrichment
                enriched_info = base_info.copy()

                # ê·¸ë˜í”„ ì •ë³´ ì¶”ê°€
                if self.relationship_graph.has_node(term):
                    neighbors = list(self.relationship_graph.neighbors(term))
                    enriched_info['graph_neighbors'] = neighbors[:10]
                    enriched_info['graph_degree'] = self.relationship_graph.degree(
                        term)

                # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
                for cluster_name, cluster_data in self.semantic_clusters.items():
                    if term in cluster_data['terms']:
                        enriched_info['semantic_cluster'] = cluster_name
                        enriched_info['cluster_size'] = cluster_data['size']
                        break

                # ë„ë©”ì¸ ì§€ì‹ ì •ë³´ ì¶”ê°€
                domain_info = {}
                for pattern_type, patterns in self.domain_knowledge_base.items():
                    if isinstance(patterns, dict) and term in patterns:
                        domain_info[pattern_type] = patterns[term]

                if domain_info:
                    enriched_info['domain_knowledge'] = domain_info

                return enriched_info

            return None
        except Exception as e:
            print(f"âš ï¸ ìš©ì–´ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def search_by_category(self, category: str, limit: int = 20) -> List[Dict]:
        """ë¶„ë¥˜ë³„ ìš©ì–´ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        try:
            if category in self.category_index:
                terms = self.category_index[category]

                # ìš©ì–´ë³„ ì ìˆ˜ ê³„ì‚° (ê·¸ë˜í”„ ì¤‘ì‹¬ì„±, í´ëŸ¬ìŠ¤í„° í¬ê¸° ë“± ê³ ë ¤)
                scored_terms = []
                for term_data in terms:
                    term_name = term_data.get('ìš©ì–´ëª…', '')
                    score = 1.0  # ê¸°ë³¸ ì ìˆ˜

                    # ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ê³ ë ¤
                    if self.relationship_graph.has_node(term_name):
                        degree = self.relationship_graph.degree(term_name)
                        score += degree * 0.1

                    # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê³ ë ¤
                    for cluster_data in self.semantic_clusters.values():
                        if term_name in cluster_data['terms']:
                            score += cluster_data['size'] * 0.05
                            break

                    scored_terms.append((term_data, score))

                # ì ìˆ˜ìˆœ ì •ë ¬
                scored_terms.sort(key=lambda x: x[1], reverse=True)

                return [term_data for term_data, _ in scored_terms[:limit]]

            return []
        except Exception as e:
            print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def fuzzy_search(self, query: str, threshold: float = 0.6) -> List[Tuple[str, float]]:
        """ê³ ê¸‰ ìœ ì‚¬ ìš©ì–´ ê²€ìƒ‰"""
        matches = []

        try:
            for term in self.search_index.keys():
                if term == query:
                    continue

                similarity = self._calculate_term_similarity(query, term)

                if similarity >= threshold:
                    matches.append((term, similarity))

            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            matches.sort(key=lambda x: x[1], reverse=True)
            return matches[:20]  # ìƒìœ„ 20ê°œ

        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _calculate_term_similarity(self, term1: str, term2: str) -> float:
        """ìš©ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¤ì°¨ì›)"""
        similarity = 0.0

        # 1. ë¬¸ì ê²¹ì¹¨ ìœ ì‚¬ë„
        common_chars = set(term1) & set(term2)
        char_similarity = len(common_chars) / max(len(term1), len(term2))
        similarity += char_similarity * 0.4

        # 2. ê¸¸ì´ ìœ ì‚¬ë„
        length_diff = abs(len(term1) - len(term2))
        max_length = max(len(term1), len(term2))
        length_similarity = 1.0 - (length_diff / max_length)
        similarity += length_similarity * 0.2

        # 3. ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„
        term1_data = self.search_index.get(term1)
        term2_data = self.search_index.get(term2)

        if term1_data and term2_data:
            cat1 = term1_data.get('ë¶„ë¥˜', '')
            cat2 = term2_data.get('ë¶„ë¥˜', '')

            if cat1 == cat2:
                similarity += 0.3
            elif cat1 and cat2:
                # ê´€ë ¨ ì¹´í…Œê³ ë¦¬ í™•ì¸
                related_categories = {
                    'ë³‘ì¦': ['ì¦ìƒ', 'ì§•í›„'],
                    'ì²˜ë°©': ['ì¹˜ë²•'],
                    'ì•½ë¬¼': ['ë³¸ì´ˆ']
                }

                if cat2 in related_categories.get(cat1, []):
                    similarity += 0.15

        # 4. ê·¸ë˜í”„ ê±°ë¦¬ ìœ ì‚¬ë„
        if (self.relationship_graph.has_node(term1) and
                self.relationship_graph.has_node(term2)):
            try:
                distance = nx.shortest_path_length(
                    self.relationship_graph, term1, term2)
                if distance <= 3:
                    graph_similarity = 1.0 / (distance + 1)
                    similarity += graph_similarity * 0.1
            except nx.NetworkXNoPath:
                pass

        return min(similarity, 1.0)

    def get_statistics(self) -> Dict:
        """ê³ ê¸‰ ìš©ì–´ì§‘ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            stats = {
                'total_terms': len(self.terms_data),
                'search_index_size': len(self.search_index),
                'categories': len(self.category_index),
                'terms_with_synonyms': len(self.synonym_index),
                'terms_with_hierarchy': len(self.hierarchical_index),
                'relationship_graph_nodes': self.relationship_graph.number_of_nodes(),
                'relationship_graph_edges': self.relationship_graph.number_of_edges(),
                'semantic_clusters': len(self.semantic_clusters),
                'domain_knowledge_patterns': len(self.domain_knowledge_base),
                'expansion_patterns': len(self.expansion_patterns)
            }

            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            category_distribution = {}
            for category, terms in self.category_index.items():
                category_distribution[category] = len(terms)

            stats['category_distribution'] = category_distribution

            # ê·¸ë˜í”„ í†µê³„
            if self.relationship_graph.number_of_nodes() > 0:
                stats['graph_density'] = nx.density(self.relationship_graph)
                stats['average_clustering'] = nx.average_clustering(
                    self.relationship_graph)

                # ì¤‘ì‹¬ì„± ë†’ì€ ìš©ì–´ë“¤
                centrality = nx.degree_centrality(self.relationship_graph)
                top_central_terms = sorted(
                    centrality.items(), key=lambda x: x[1], reverse=True)[:5]
                stats['most_central_terms'] = [
                    term for term, _ in top_central_terms]

            return stats

        except Exception as e:
            print(f"âš ï¸ í†µê³„ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def analyze_query_patterns(self, queries: List[str]) -> Dict:
        """ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'total_queries': len(queries),
            'unique_queries': len(set(queries)),
            'category_distribution': Counter(),
            'length_distribution': Counter(),
            'expansion_effectiveness': {},
            'common_patterns': []
        }

        for query in queries:
            # ê¸¸ì´ ë¶„í¬
            analysis['length_distribution'][len(query)] += 1

            # ì¹´í…Œê³ ë¦¬ ë¶„í¬
            if query in self.search_index:
                category = self.search_index[query].get('ë¶„ë¥˜', 'ê¸°íƒ€')
                analysis['category_distribution'][category] += 1

            # í™•ì¥ íš¨ê³¼ ë¶„ì„
            expansions = self.expand_query(query, max_expansions=5)
            analysis['expansion_effectiveness'][query] = len(expansions)

        # ê³µí†µ íŒ¨í„´ ì¶”ì¶œ
        pattern_counter = Counter()
        for query in queries:
            if len(query) >= 2:
                for i in range(len(query) - 1):
                    pattern = query[i:i + 2]
                    pattern_counter[pattern] += 1

        analysis['common_patterns'] = pattern_counter.most_common(10)

        return analysis

    def export_knowledge_graph(self, format: str = 'gexf') -> str:
        """ì§€ì‹ ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸°"""
        try:
            output_file = self.cache_path / f'knowledge_graph.{format}'

            if format == 'gexf':
                nx.write_gexf(self.relationship_graph, output_file)
            elif format == 'gml':
                nx.write_gml(self.relationship_graph, output_file)
            elif format == 'graphml':
                nx.write_graphml(self.relationship_graph, output_file)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")

            return str(output_file)

        except Exception as e:
            print(f"âš ï¸ ì§€ì‹ ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def clear_cache(self):
        """ìºì‹œ íŒŒì¼ ì‚­ì œ"""
        try:
            if self.cache_file.exists():
                self.cache_file.unlink()
                print("ğŸ—‘ï¸ í‘œì¤€ìš©ì–´ì§‘ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("ğŸ’­ ì‚­ì œí•  ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def rebuild_index(self):
        """ê°•ì œë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        try:
            print("ğŸ”„ í‘œì¤€ìš©ì–´ì§‘ ê³ ê¸‰ ì¸ë±ìŠ¤ ê°•ì œ ì¬êµ¬ì¶• ì¤‘...")
            self._build_index()
            self._build_relationship_graph()
            self._analyze_semantic_clusters()
            self._build_domain_knowledge_base()
            self._save_cache()
            print("âœ… ê³ ê¸‰ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {e}")

    def validate_system_integrity(self) -> Dict:
        """ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì¦"""
        validation = {
            'basic_indexes_ok': True,
            'graph_ok': True,
            'clusters_ok': True,
            'domain_knowledge_ok': True,
            'errors': [],
            'warnings': []
        }

        try:
            # ê¸°ë³¸ ì¸ë±ìŠ¤ ê²€ì¦
            if not self.search_index:
                validation['basic_indexes_ok'] = False
                validation['errors'].append("ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            # ê·¸ë˜í”„ ê²€ì¦
            if self.relationship_graph.number_of_nodes() == 0:
                validation['graph_ok'] = False
                validation['warnings'].append("ê´€ê³„ ê·¸ë˜í”„ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            # í´ëŸ¬ìŠ¤í„° ê²€ì¦
            if not self.semantic_clusters:
                validation['clusters_ok'] = False
                validation['warnings'].append("ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            # ë„ë©”ì¸ ì§€ì‹ ê²€ì¦
            if not self.domain_knowledge_base:
                validation['domain_knowledge_ok'] = False
                validation['warnings'].append("ë„ë©”ì¸ ì§€ì‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
            inconsistencies = self._check_data_consistency()
            if inconsistencies:
                validation['warnings'].extend(inconsistencies)

        except Exception as e:
            validation['errors'].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")

        return validation

    def _check_data_consistency(self) -> List[str]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬"""
        issues = []

        # ê²€ìƒ‰ ì¸ë±ìŠ¤ì™€ ì›ë³¸ ë°ì´í„° ì¼ì¹˜ í™•ì¸
        expected_terms = set()
        for term_data in self.terms_data:
            term_name = term_data.get('ìš©ì–´ëª…', '')
            if term_name:
                expected_terms.add(term_name)

        actual_terms = set(term for term in self.search_index.keys())

        if len(expected_terms) > len(actual_terms) * 0.8:  # 80% ì´ìƒ ì¼ì¹˜í•´ì•¼ í•¨
            missing = expected_terms - actual_terms
            if missing:
                issues.append(f"ê²€ìƒ‰ ì¸ë±ìŠ¤ì—ì„œ ëˆ„ë½ëœ ìš©ì–´: {len(missing)}ê°œ")

        return issues


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_terms_manager() -> MedicalTermsManager:
    """í‘œì¤€ìš©ì–´ì§‘ ê´€ë¦¬ì ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return MedicalTermsManager()


def test_advanced_terms_manager():
    """ê³ ê¸‰ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜"""
    print("ğŸ§ª ê³ ê¸‰ í‘œì¤€ìš©ì–´ì§‘ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸")

    manager = MedicalTermsManager()

    # ê¸°ë³¸ í†µê³„ ì •ë³´ ì¶œë ¥
    stats = manager.get_statistics()
    print(f"\nğŸ“Š ê³ ê¸‰ ì‹œìŠ¤í…œ í†µê³„:")
    print(f"   ì´ ìš©ì–´ ìˆ˜: {stats.get('total_terms', 0):,}ê°œ")
    print(f"   ê²€ìƒ‰ ì¸ë±ìŠ¤: {stats.get('search_index_size', 0):,}ê°œ")
    print(f"   ì¹´í…Œê³ ë¦¬: {stats.get('categories', 0)}ê°œ")
    print(f"   ê´€ê³„ ê·¸ë˜í”„ ë…¸ë“œ: {stats.get('relationship_graph_nodes', 0):,}ê°œ")
    print(f"   ê´€ê³„ ê·¸ë˜í”„ ì—£ì§€: {stats.get('relationship_graph_edges', 0):,}ê°œ")
    print(f"   ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°: {stats.get('semantic_clusters', 0)}ê°œ")
    print(f"   ë„ë©”ì¸ ì§€ì‹ íŒ¨í„´: {stats.get('domain_knowledge_patterns', 0)}ê°œ")

    # ê·¸ë˜í”„ í†µê³„
    if 'graph_density' in stats:
        print(f"   ê·¸ë˜í”„ ë°€ë„: {stats['graph_density']:.4f}")
        print(f"   í‰ê·  í´ëŸ¬ìŠ¤í„°ë§: {stats['average_clustering']:.4f}")
        print(
            f"   ì¤‘ì‹¬ì„± ë†’ì€ ìš©ì–´: {', '.join(stats.get('most_central_terms', []))}")

    # ê³ ê¸‰ ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸
    test_queries = ['è¡€è™›', 'å››å›å­æ¹¯', 'äººåƒ', 'é™°è™›', 'è£œä¸­ç›Šæ°£æ¹¯']

    print(f"\nğŸ” ê³ ê¸‰ ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸:")
    for query in test_queries:
        print(f"\n   ğŸ“ '{query}' í™•ì¥ í…ŒìŠ¤íŠ¸:")

        # ê¸°ë³¸ í™•ì¥
        expansions = manager.expand_query(query, max_expansions=8)
        print(f"      í™•ì¥ ê²°ê³¼ ({len(expansions)}ê°œ): {', '.join(expansions)}")

        # ê´€ë ¨ ìš©ì–´
        related = manager.get_related_terms(query, max_terms=5)
        if related:
            print(f"      ê´€ë ¨ ìš©ì–´: {', '.join(related)}")

        # ìš©ì–´ ì •ë³´
        term_info = manager.get_term_info(query)
        if term_info:
            category = term_info.get('ë¶„ë¥˜', 'ë¯¸ë¶„ë¥˜')
            print(f"      ë¶„ë¥˜: {category}")

            if 'graph_degree' in term_info:
                print(f"      ê·¸ë˜í”„ ì—°ê²°ë„: {term_info['graph_degree']}")

            if 'semantic_cluster' in term_info:
                print(f"      ì†Œì† í´ëŸ¬ìŠ¤í„°: {term_info['semantic_cluster']}")

    # ìœ ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” ìœ ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ('è¡€è™›'ì™€ ìœ ì‚¬í•œ ìš©ì–´):")
    similar_terms = manager.fuzzy_search('è¡€è™›', threshold=0.3)
    for term, similarity in similar_terms[:5]:
        print(f"      {term}: {similarity:.3f}")

    # ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì¦
    print(f"\nğŸ”§ ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì¦:")
    validation = manager.validate_system_integrity()

    status_symbols = {True: "âœ…", False: "âŒ"}
    print(f"   ê¸°ë³¸ ì¸ë±ìŠ¤: {status_symbols[validation['basic_indexes_ok']]}")
    print(f"   ê´€ê³„ ê·¸ë˜í”„: {status_symbols[validation['graph_ok']]}")
    print(f"   ì˜ë¯¸ í´ëŸ¬ìŠ¤í„°: {status_symbols[validation['clusters_ok']]}")
    print(f"   ë„ë©”ì¸ ì§€ì‹: {status_symbols[validation['domain_knowledge_ok']]}")

    if validation['errors']:
        print(f"   âš ï¸ ì˜¤ë¥˜: {', '.join(validation['errors'])}")
    if validation['warnings']:
        print(f"   ğŸ’¡ ê²½ê³ : {', '.join(validation['warnings'])}")

    # ë„ë©”ì¸ ì§€ì‹ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§  ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸:")
    if manager.domain_knowledge_base:
        deficiency_patterns = manager.domain_knowledge_base.get(
            'deficiency_patterns', {})
        if deficiency_patterns:
            print(f"   í—ˆì¦ íŒ¨í„´: {len(deficiency_patterns)}ê°œ")
            for pattern_name in list(deficiency_patterns.keys())[:3]:
                pattern_data = deficiency_patterns[pattern_name]
                herbs = pattern_data.get('primary_herbs', [])
                prescriptions = pattern_data.get('primary_prescriptions', [])
                print(
                    f"      {pattern_name}: ì£¼ìš”ì•½ì¬ {len(herbs)}ê°œ, ì£¼ìš”ì²˜ë°© {len(prescriptions)}ê°œ")


def benchmark_expansion_performance():
    """í™•ì¥ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time

    print("âš¡ í™•ì¥ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")

    manager = MedicalTermsManager()

    test_queries = [
        'è¡€è™›', 'æ°£è™›', 'é™°è™›', 'é™½è™›', 'å››ç‰©æ¹¯', 'å››å›å­æ¹¯', 'å…­å›å­æ¹¯',
        'è£œä¸­ç›Šæ°£æ¹¯', 'ç•¶æ­¸è£œè¡€æ¹¯', 'äººåƒ', 'ç•¶æ­¸', 'é»ƒèŠª', 'ç™½æœ®', 'èŒ¯è‹“',
        'å¿ƒæ‚¸', 'å¤±çœ ', 'çœ©æšˆ', 'é ­ç—›', 'èƒ¸ç—›', 'è…¹ç—›'
    ]

    # ë‹¨ì¼ í™•ì¥ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š ë‹¨ì¼ ì¿¼ë¦¬ í™•ì¥ ì„±ëŠ¥:")
    total_time = 0
    total_expansions = 0

    for query in test_queries[:10]:  # ìƒìœ„ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        expansions = manager.expand_query(query, max_expansions=10)
        end_time = time.time()

        query_time = end_time - start_time
        total_time += query_time
        total_expansions += len(expansions)

        print(f"   {query}: {len(expansions)}ê°œ í™•ì¥, {query_time:.4f}ì´ˆ")

    avg_time = total_time / len(test_queries[:10])
    avg_expansions = total_expansions / len(test_queries[:10])

    print(f"\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
    print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.4f}ì´ˆ")
    print(f"   í‰ê·  í™•ì¥ ìˆ˜: {avg_expansions:.1f}ê°œ")
    print(f"   ì´ˆë‹¹ ì²˜ë¦¬ ê°€ëŠ¥ ì¿¼ë¦¬: {1 / avg_time:.1f}ê°œ")

    # ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ (20ê°œ ì¿¼ë¦¬):")
    start_time = time.time()

    batch_results = []
    for query in test_queries:
        expansions = manager.expand_query(query, max_expansions=5)
        batch_results.append((query, expansions))

    batch_time = time.time() - start_time

    print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {batch_time:.4f}ì´ˆ")
    print(f"   í‰ê·  ì¿¼ë¦¬ë‹¹ ì‹œê°„: {batch_time / len(test_queries):.4f}ì´ˆ")
    print(f"   ì²˜ë¦¬ëŸ‰: {len(test_queries) / batch_time:.1f} ì¿¼ë¦¬/ì´ˆ")


def export_analysis_report(manager: MedicalTermsManager, output_path: str = None):
    """ë¶„ì„ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°"""
    if output_path is None:
        output_path = manager.cache_path / 'analysis_report.txt'

    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê³ ê¸‰ ë¶„ì„ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n")
            f.write(
                f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # ê¸°ë³¸ í†µê³„
            stats = manager.get_statistics()
            f.write("ğŸ“Š ê¸°ë³¸ í†µê³„\n")
            f.write("-" * 40 + "\n")
            for key, value in stats.items():
                if isinstance(value, dict):
                    f.write(f"{key}:\n")
                    for sub_key, sub_value in value.items():
                        f.write(f"  {sub_key}: {sub_value}\n")
                else:
                    f.write(f"{key}: {value}\n")
            f.write("\n")

            # ë„ë©”ì¸ ì§€ì‹ ë¶„ì„
            f.write("ğŸ§  ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ ë¶„ì„\n")
            f.write("-" * 40 + "\n")

            deficiency_patterns = manager.domain_knowledge_base.get(
                'deficiency_patterns', {})
            f.write(f"í—ˆì¦ íŒ¨í„´: {len(deficiency_patterns)}ê°œ\n")

            for pattern_name, pattern_data in deficiency_patterns.items():
                f.write(f"\n{pattern_name}:\n")
                f.write(
                    f"  ì£¼ìš” ì•½ì¬: {', '.join(pattern_data.get('primary_herbs', []))}\n")
                f.write(
                    f"  ì£¼ìš” ì²˜ë°©: {', '.join(pattern_data.get('primary_prescriptions', []))}\n")
                f.write(
                    f"  ê´€ë ¨ ì¦ìƒ: {', '.join(pattern_data.get('related_symptoms', []))}\n")

            # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„
            f.write(f"\nğŸ”¬ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(manager.semantic_clusters)}\n")

            for cluster_name, cluster_data in manager.semantic_clusters.items():
                f.write(f"\n{cluster_name}:\n")
                f.write(f"  í¬ê¸°: {cluster_data['size']}ê°œ ìš©ì–´\n")
                f.write(f"  ì§€ë°°ì  ì¹´í…Œê³ ë¦¬: {cluster_data['dominant_category']}\n")
                f.write(f"  ìš©ì–´ë“¤: {', '.join(cluster_data['terms'][:10])}\n")
                if len(cluster_data['terms']) > 10:
                    f.write(f"  ... ì™¸ {len(cluster_data['terms']) - 10}ê°œ\n")

            # ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì¦
            f.write(f"\nğŸ”§ ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì¦\n")
            f.write("-" * 40 + "\n")

            validation = manager.validate_system_integrity()
            f.write(
                f"ê¸°ë³¸ ì¸ë±ìŠ¤: {'ì •ìƒ' if validation['basic_indexes_ok'] else 'ì˜¤ë¥˜'}\n")
            f.write(f"ê´€ê³„ ê·¸ë˜í”„: {'ì •ìƒ' if validation['graph_ok'] else 'ì˜¤ë¥˜'}\n")
            f.write(
                f"ì˜ë¯¸ í´ëŸ¬ìŠ¤í„°: {'ì •ìƒ' if validation['clusters_ok'] else 'ì˜¤ë¥˜'}\n")
            f.write(
                f"ë„ë©”ì¸ ì§€ì‹: {'ì •ìƒ' if validation['domain_knowledge_ok'] else 'ì˜¤ë¥˜'}\n")

            if validation['errors']:
                f.write(f"\nì˜¤ë¥˜ ëª©ë¡:\n")
                for error in validation['errors']:
                    f.write(f"  - {error}\n")

            if validation['warnings']:
                f.write(f"\nê²½ê³  ëª©ë¡:\n")
                for warning in validation['warnings']:
                    f.write(f"  - {warning}\n")

        print(f"ğŸ“„ ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
        return str(output_path)

    except Exception as e:
        print(f"âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None


if __name__ == "__main__":
    print("ğŸš€ ê³ ê¸‰ í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    test_advanced_terms_manager()

    print("\n" + "=" * 60)

    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    benchmark_expansion_performance()

    print("\n" + "=" * 60)

    # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    manager = create_terms_manager()
    report_path = export_analysis_report(manager)

    if report_path:
        print(f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ìƒì„¸ ë¦¬í¬íŠ¸: {report_path}")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
