#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ì˜ë³´ê° ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ - document_processor_improved.py (ê°œì„ ëœ ë²„ì „)
í•˜ë“œì½”ë”©ëœ TCM ìš©ì–´ ì‚¬ì „ì„ í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê¸°ë°˜ìœ¼ë¡œ êµì²´
ë¬¸ì„œ ë¡œë”©, íŒŒì‹±, ì²­í‚¹ì„ ë‹´ë‹¹
"""

import os
import re
import unicodedata
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings("ignore")

try:
    import jieba
    import opencc
    import tiktoken
except ImportError as e:
    print(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    raise


class DocumentProcessor:
    def __init__(self, data_path: str, terms_manager=None):
        """ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        self.data_path = Path(data_path)
        self.terms_manager = terms_manager

        # ì¤‘êµ­ì–´ ì²˜ë¦¬ ë„êµ¬ ì´ˆê¸°í™”
        self.cc = opencc.OpenCC('t2s')

        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (GPT-4 í˜¸í™˜)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # ë™ì  ìš©ì–´ ì‚¬ì „ (í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜)
        self.dynamic_tcm_terms = {}
        self.category_patterns = {}
        self.prescription_patterns = set()
        self.herb_patterns = set()

        # ì´ˆê¸°í™”
        self.setup_chinese_tools()
        self._build_dynamic_tcm_dictionary()

    def setup_chinese_tools(self):
        """ì¤‘êµ­ì–´ ì²˜ë¦¬ ë„êµ¬ ì„¤ì • (í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜)"""
        print("ğŸ”§ ì¤‘êµ­ì–´ ì²˜ë¦¬ ë„êµ¬ ì„¤ì • ì¤‘...")

        try:
            if self.terms_manager and hasattr(self.terms_manager, 'search_index'):
                print("ğŸ“š í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê¸°ë°˜ jieba ì„¤ì • ì¤‘...")
                added_count = 0

                # í‘œì¤€ìš©ì–´ì§‘ì˜ ëª¨ë“  ìš©ì–´ë¥¼ jiebaì— ì¶”ê°€
                for term in self.terms_manager.search_index.keys():
                    if len(term) >= 2 and self._is_chinese_term(term):
                        jieba.add_word(term, freq=None, tag=None)
                        added_count += 1

                # ì¶”ê°€ë¡œ ìš©ì–´ì§‘ì—ì„œ ê³ ë¹ˆë„ ìš©ì–´ë“¤ ìš°ì„  ì²˜ë¦¬
                high_priority_categories = ['ì²˜ë°©', 'ì•½ë¬¼', 'ë³‘ì¦', 'ìƒë¦¬', 'ë³‘ë¦¬']
                for category in high_priority_categories:
                    try:
                        category_terms = self.terms_manager.search_by_category(
                            category, limit=100)
                        for term_data in category_terms:
                            term_name = term_data.get('ìš©ì–´ëª…', '')
                            hanja_name = term_data.get('ìš©ì–´ëª…_í•œì', '')

                            if term_name and len(term_name) >= 2:
                                jieba.add_word(
                                    term_name, freq=10, tag=category)
                            if hanja_name and len(hanja_name) >= 2:
                                jieba.add_word(
                                    hanja_name, freq=20, tag=category)

                            # ë™ì˜ì–´ë„ ì¶”ê°€
                            synonyms = term_data.get('ë™ì˜ì–´', [])
                            for synonym in synonyms:
                                if synonym and len(synonym) >= 2:
                                    jieba.add_word(
                                        synonym, freq=5, tag=category)
                    except Exception as e:
                        print(f"âš ï¸ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                print(f"âœ… {added_count}ê°œ í‘œì¤€ìš©ì–´ jieba ë“±ë¡ ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ jieba ì„¤ì • ì‹¤íŒ¨: {e}")
            self._setup_fallback_jieba_terms()

    def _is_chinese_term(self, term: str) -> bool:
        """í•œì ìš©ì–´ì¸ì§€ í™•ì¸"""
        if not term:
            return False
        return all('\u4e00' <= char <= '\u9fff' for char in term)

    def _setup_fallback_jieba_terms(self):
        """í´ë°±: ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ë¥¼ jiebaì— ì¶”ê°€"""
        print("ğŸ“š ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ë¡œ jieba ì„¤ì • ì¤‘...")

        basic_terms = [
            # ì²˜ë°©
            "å››ç‰©æ¹¯", "å…­å›å­æ¹¯", "è£œä¸­ç›Šæ°£æ¹¯", "ç•¶æ­¸è£œè¡€æ¹¯", "çŠ€è§’åœ°é»ƒæ¹¯",
            "å…«ç‰©æ¹¯", "åå…¨å¤§è£œæ¹¯", "äººåƒé¤Šæ¦®æ¹¯", "æ­¸è„¾æ¹¯", "ç”˜éº¥å¤§æ£—æ¹¯",

            # ì•½ì¬
            "äººåƒ", "ç•¶æ­¸", "å·èŠ", "ç™½èŠ", "ç†Ÿåœ°é»ƒ", "ç”Ÿåœ°é»ƒ", "é»ƒèŠª", "ç™½æœ®",
            "èŒ¯è‹“", "ç”˜è‰", "é™³çš®", "åŠå¤", "æ³å¯¦", "åšæœ´", "æ¡”æ¢—", "æä»",
            "éº¥é–€å†¬", "äº”å‘³å­", "å±±è—¥", "èŒ¯ç¥", "é å¿—", "çŸ³è–è’²", "æœ±ç ‚", "é¾éª¨",

            # ë³‘ì¦
            "é©šæ‚¸", "å¥å¿˜", "ç™²ç™‡", "çœ©æšˆ", "å¤±çœ ", "è™›å‹", "è¡€è™›", "æ°£è™›",
            "é™°è™›", "é™½è™›", "è„¾èƒƒè™›å¼±", "å¿ƒè…ä¸äº¤", "è‚é¬±æ°£æ»¯", "ç—°æ¿•é˜»çµ¡",

            # ì´ë¡ 
            "é™°é™½", "äº”è¡Œ", "è‡Ÿè…‘", "æ°£è¡€", "ç¶“çµ¡", "ç²¾æ°£ç¥", "å›è‡£ä½ä½¿",
            "ç‡Ÿè¡›", "ä¸‰ç„¦", "å‘½é–€", "å…ƒæ°£", "çœŸé™°", "çœŸé™½", "å…ˆå¤©ä¹‹æœ¬"
        ]

        for term in basic_terms:
            jieba.add_word(term, freq=10)
        print(f"âœ… {len(basic_terms)}ê°œ ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ ì¶”ê°€ ì™„ë£Œ")

    def _build_dynamic_tcm_dictionary(self):
        """í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ë™ì  TCM ì‚¬ì „ êµ¬ì¶•"""
        print("ğŸ”¨ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ë™ì  TCM ì‚¬ì „ êµ¬ì¶• ì¤‘...")

        if not self.terms_manager:
            self._build_fallback_tcm_dictionary()
            return

        try:
            # ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ì§‘
            categories_mapping = {
                'symptoms': ['ë³‘ì¦', 'ì¦ìƒ', 'ì§•í›„'],
                'herbs': ['ì•½ë¬¼'],
                'prescriptions': ['ì²˜ë°©'],
                'theories': ['ìƒë¦¬', 'ë³‘ë¦¬', 'ë³€ì¦'],
                'methods': ['ì¹˜ë²•', 'ì¹¨êµ¬'],
                'diagnostics': ['ì§„ì°°']
            }

            self.dynamic_tcm_terms = {}
            total_terms = 0

            for key, categories in categories_mapping.items():
                terms_list = []

                for category in categories:
                    try:
                        category_terms = self.terms_manager.search_by_category(
                            category, limit=200)
                        for term_data in category_terms:
                            # í•œìëª… ìš°ì„ , ì—†ìœ¼ë©´ ìš©ì–´ëª…
                            hanja = term_data.get('ìš©ì–´ëª…_í•œì', '')
                            hangul = term_data.get('ìš©ì–´ëª…', '')

                            if hanja and len(hanja) >= 2:
                                terms_list.append(hanja)
                            elif hangul and len(hangul) >= 2:
                                terms_list.append(hangul)

                            # ë™ì˜ì–´ë„ í¬í•¨
                            synonyms = term_data.get('ë™ì˜ì–´', [])
                            for synonym in synonyms:
                                if synonym and len(synonym) >= 2:
                                    terms_list.append(synonym)

                    except Exception as e:
                        print(f"âš ï¸ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
                self.dynamic_tcm_terms[key] = list(set(terms_list))
                total_terms += len(self.dynamic_tcm_terms[key])

            # íŠ¹ë³„ íŒ¨í„´ êµ¬ì¶•
            self._build_prescription_patterns()
            self._build_herb_patterns()

            print(f"âœ… ë™ì  TCM ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: ì´ {total_terms}ê°œ ìš©ì–´")
            for key, terms in self.dynamic_tcm_terms.items():
                print(f"   {key}: {len(terms)}ê°œ")

        except Exception as e:
            print(f"âš ï¸ ë™ì  TCM ì‚¬ì „ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self._build_fallback_tcm_dictionary()

    def _build_prescription_patterns(self):
        """ì²˜ë°© íŒ¨í„´ ë™ì  êµ¬ì¶•"""
        try:
            prescriptions = self.terms_manager.search_by_category(
                'ì²˜ë°©', limit=300)

            for prescription in prescriptions:
                hanja = prescription.get('ìš©ì–´ëª…_í•œì', '')
                if hanja:
                    # ì²˜ë°© ì ‘ë¯¸ì‚¬ ì¶”ì¶œ
                    for suffix in ['æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'é£²', 'ä¸¹', 'æ–¹', 'å­']:
                        if hanja.endswith(suffix):
                            self.prescription_patterns.add(suffix)

                    # ì „ì²´ ì²˜ë°©ëª…ë„ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€
                    if len(hanja) >= 3:
                        self.prescription_patterns.add(hanja)

            print(f"ğŸ“Š ì²˜ë°© íŒ¨í„´ {len(self.prescription_patterns)}ê°œ êµ¬ì¶•")

        except Exception as e:
            print(f"âš ï¸ ì²˜ë°© íŒ¨í„´ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.prescription_patterns = {'æ¹¯', 'æ•£', 'ä¸¸', 'è†'}

    def _build_herb_patterns(self):
        """ì•½ì¬ íŒ¨í„´ ë™ì  êµ¬ì¶•"""
        try:
            herbs = self.terms_manager.search_by_category('ì•½ë¬¼', limit=200)

            for herb in herbs:
                hanja = herb.get('ìš©ì–´ëª…_í•œì', '')
                if hanja and len(hanja) >= 2:
                    self.herb_patterns.add(hanja)

            print(f"ğŸ“Š ì•½ì¬ íŒ¨í„´ {len(self.herb_patterns)}ê°œ êµ¬ì¶•")

        except Exception as e:
            print(f"âš ï¸ ì•½ì¬ íŒ¨í„´ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.herb_patterns = {'äººåƒ', 'ç•¶æ­¸', 'å·èŠ',
                                  'ç™½èŠ', 'ç†Ÿåœ°é»ƒ', 'é»ƒèŠª', 'ç™½æœ®', 'èŒ¯è‹“', 'ç”˜è‰'}

    def _build_fallback_tcm_dictionary(self):
        """í´ë°±: ê¸°ë³¸ TCM ìš©ì–´ ì‚¬ì „"""
        print("ğŸ“š ê¸°ë³¸ TCM ìš©ì–´ ì‚¬ì „ìœ¼ë¡œ ì´ˆê¸°í™”")

        self.dynamic_tcm_terms = {
            "symptoms": [
                "é©šæ‚¸", "å¥å¿˜", "ç™²ç™‡", "çœ©æšˆ", "å¤±çœ ", "è™›å‹", "è¡€è™›", "æ°£è™›", "é™°è™›", "é™½è™›",
                "å¿ƒæ‚¸", "ä¸å¯", "é ­ç—›", "è…¹ç—›", "èƒ¸ç—›", "è„…ç—›", "è…°ç—›", "é—œç¯€ç—›", "è‚Œè‚‰ç—›",
                "ç™¼ç†±", "æƒ¡å¯’", "è‡ªæ±—", "ç›œæ±—", "å’³å—½", "å–˜æ¯", "å˜”å", "æ³„ç€‰", "ä¾¿ç§˜"
            ],
            "herbs": [
                "äººåƒ", "ç•¶æ­¸", "å·èŠ", "ç™½èŠ", "ç†Ÿåœ°é»ƒ", "ç”Ÿåœ°é»ƒ", "é»ƒèŠª", "ç™½æœ®", "èŒ¯è‹“", "ç”˜è‰",
                "é™³çš®", "åŠå¤", "æ³å¯¦", "åšæœ´", "æ¡”æ¢—", "æä»", "éº¥é–€å†¬", "äº”å‘³å­", "å±±è—¥", "èŒ¯ç¥",
                "é å¿—", "çŸ³è–è’²", "æœ±ç ‚", "é¾éª¨", "ç‰¡è £", "é…¸æ£—ä»", "æŸå­ä»", "é˜¿è† ", "åœ°éª¨çš®"
            ],
            "prescriptions": [
                "å››ç‰©æ¹¯", "å…­å›å­æ¹¯", "è£œä¸­ç›Šæ°£æ¹¯", "ç•¶æ­¸è£œè¡€æ¹¯", "çŠ€è§’åœ°é»ƒæ¹¯", "å…«ç‰©æ¹¯", "åå…¨å¤§è£œæ¹¯",
                "äººåƒé¤Šæ¦®æ¹¯", "æ­¸è„¾æ¹¯", "ç”˜éº¥å¤§æ£—æ¹¯", "é€é™æ•£", "æŸ´èƒ¡ç–è‚æ•£", "å¹³èƒƒæ•£", "äºŒé™³æ¹¯"
            ],
            "theories": [
                "é™°é™½", "äº”è¡Œ", "è‡Ÿè…‘", "æ°£è¡€", "ç¶“çµ¡", "ç²¾æ°£ç¥", "å›è‡£ä½ä½¿", "ç‡Ÿè¡›", "ä¸‰ç„¦",
                "å‘½é–€", "å…ƒæ°£", "çœŸé™°", "çœŸé™½", "å…ˆå¤©ä¹‹æœ¬", "å¾Œå¤©ä¹‹æœ¬", "è…é–“å‹•æ°£"
            ],
            "methods": [
                "æ±—æ³•", "åæ³•", "ä¸‹æ³•", "å’Œæ³•", "æº«æ³•", "æ·¸æ³•", "è£œæ³•", "æ¶ˆæ³•", "é‡åˆº", "è‰¾ç¸"
            ],
            "diagnostics": [
                "æœ›è¨º", "èè¨º", "å•è¨º", "åˆ‡è¨º", "å››è¨ºåˆåƒ", "å…«ç¶±è¾¨è­‰", "è‡Ÿè…‘è¾¨è­‰", "ç¶“çµ¡è¾¨è­‰"
            ]
        }

        self.prescription_patterns = {'æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'é£²', 'ä¸¹', 'æ–¹'}
        self.herb_patterns = set(self.dynamic_tcm_terms["herbs"])

    def normalize_text(self, text: str) -> Tuple[str, str]:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ë²ˆì²´ ìœ ì§€ + ê°„ì²´ ë³€í™˜)"""
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        normalized = unicodedata.normalize('NFKC', text)
        # ê³µë°± ì •ë¦¬
        normalized = re.sub(r'[ã€€\s]+', ' ', normalized)
        # ê°„ì²´ ë³€í™˜ (ê²€ìƒ‰ í™•ì¥ìš©)
        simplified = self.cc.convert(normalized)
        return normalized, simplified

    def calculate_data_hash(self) -> str:
        """ë°ì´í„° ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        file_info = []

        for root, dirs, files in os.walk(self.data_path):
            for file in sorted(files):
                if file.endswith('.txt'):
                    file_path = Path(root) / file
                    stat = file_path.stat()
                    file_info.append(f"{file}:{stat.st_mtime}:{stat.st_size}")

        hash_md5.update('|'.join(file_info).encode('utf-8'))
        return hash_md5.hexdigest()

    def load_documents(self) -> List[Dict]:
        """ë™ì˜ë³´ê° ë¬¸ì„œ ë¡œë“œ"""
        print("ğŸ“š ë™ì˜ë³´ê° ë¬¸ì„œ ë¡œë”© ì¤‘...")
        all_chunks = []

        for root, dirs, files in os.walk(self.data_path):
            for file in files:
                if file.endswith('.txt'):
                    file_path = Path(root) / file
                    print(f"   ğŸ“„ {file_path.name} ì²˜ë¦¬ ì¤‘...")

                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        chunks = self.parse_document_structure(content)
                        for chunk in chunks:
                            chunk['metadata']['source_file'] = file_path.name
                            chunk['metadata']['source_path'] = str(file_path)

                        all_chunks.extend(chunks)

                    except Exception as e:
                        print(f"âš ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks

    def parse_document_structure(self, text: str) -> List[Dict]:
        """ë™ì˜ë³´ê° ë¬¸ì„œ êµ¬ì¡° íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
        lines = text.split('\n')
        chunks = []
        current_context = {
            'AA': '',  # í¸ëª…/ê¶Œëª…
            'XX': '',  # ì €ì ì •ë³´
            'BB': '',  # ëŒ€ë¶„ë¥˜
            'CC': '',  # ì¤‘ë¶„ë¥˜
            'DD': '',  # ì†Œë¶„ë¥˜
        }

        current_chunk = {'content': [], 'metadata': {}}
        current_prescription = None
        prescription_content = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # êµ¬ì¡° ë§ˆì»¤ í™•ì¸
            if len(line) > 2 and line[:2] in ['AA', 'XX', 'OO', 'ZZ', 'BB', 'CC', 'DD', 'DP', 'SS', 'PP']:
                marker = line[:2]
                content = line[2:].strip()

                # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                if marker in current_context:
                    current_context[marker] = content

                # ì²­í‚¹ ë¡œì§ (ê°œì„ ëœ ë²„ì „)
                if marker == 'BB':  # ìƒˆë¡œìš´ ëŒ€ë¶„ë¥˜
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                        current_prescription = None
                        prescription_content = []

                    if current_chunk['content']:
                        chunks.append(self.finalize_chunk(
                            current_chunk, current_context))
                    current_chunk = {'content': [
                        content], 'metadata': dict(current_context)}

                elif marker == 'CC':  # ìƒˆë¡œìš´ ì¤‘ë¶„ë¥˜
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                        current_prescription = None
                        prescription_content = []

                    if current_chunk['content'] and len(' '.join(current_chunk['content'])) > 50:
                        chunks.append(self.finalize_chunk(
                            current_chunk, current_context))
                    current_chunk = {'content': [
                        content], 'metadata': dict(current_context)}

                elif marker == 'DP':  # ì²˜ë°© ì‹œì‘
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                    current_prescription = content
                    prescription_content = []

                elif marker == 'SS':  # ì²˜ë°© ìƒì„¸
                    if current_prescription:
                        prescription_content.append(content)

                elif marker == 'ZZ':  # ë³¸ë¬¸ ë‚´ìš©
                    if not current_prescription:
                        current_chunk['content'].append(content)
                    else:
                        prescription_content.append(content)

        # ë§ˆì§€ë§‰ ì²˜ë¦¬
        if current_prescription:
            chunks.append(self.create_prescription_chunk(
                current_prescription, prescription_content, current_context))
        if current_chunk['content']:
            chunks.append(self.finalize_chunk(current_chunk, current_context))

        return chunks

    def create_prescription_chunk(self, prescription_name: str, prescription_content: List[str], context: Dict) -> Dict:
        """ì²˜ë°© ì „ìš© ì²­í¬ ìƒì„± (ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ)"""
        full_content = f"è™•æ–¹: {prescription_name}\n\n" + \
            '\n'.join(prescription_content)
        normalized_text, simplified_text = self.normalize_text(full_content)
        keywords = self.extract_prescription_keywords_improved(
            prescription_name, prescription_content)

        return {
            'content': normalized_text,
            'content_simplified': simplified_text,
            'metadata': {
                **context,
                'type': 'prescription',
                'prescription_name': prescription_name,
                'keywords': keywords,
                'token_count': len(self.tokenizer.encode(normalized_text)),
                'char_count': len(normalized_text)
            }
        }

    def extract_prescription_keywords_improved(self, prescription_name: str, content: List[str]) -> List[str]:
        """ê°œì„ ëœ ì²˜ë°© í‚¤ì›Œë“œ ì¶”ì¶œ (í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜)"""
        keywords = [prescription_name]
        full_text = ' '.join(content)

        # 1. ì•½ì¬ëª… íŒ¨í„´ ë§¤ì¹­ (ê°œì„ ëœ ì •ê·œì‹)
        herb_patterns = [
            r'([ä¸€-é¾¯]{2,4})\s*(?:å„|æ¯|ç”¨|å–|åŠ )',
            r'([ä¸€-é¾¯]{2,4})\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]\s*[éŒ¢å…©åˆ†æ–¤å‡åˆ]',
            r'([ä¸€-é¾¯]{2,4})\s*(?:å°‘è¨±|é©é‡|è‹¥å¹²)',
        ]

        extracted_herbs = set()
        for pattern in herb_patterns:
            matches = re.findall(pattern, full_text)
            for match in matches:
                if len(match) >= 2:
                    extracted_herbs.add(match)

        # 2. í‘œì¤€ìš©ì–´ì§‘ì—ì„œ ì•½ì¬ í™•ì¸ ë° ì¶”ê°€
        if self.terms_manager:
            try:
                for herb in extracted_herbs:
                    herb_info = self.terms_manager.get_term_info(herb)
                    if herb_info and herb_info.get('ë¶„ë¥˜') == 'ì•½ë¬¼':
                        keywords.append(herb)
                        # ê´€ë ¨ ì•½ì¬ë„ ì¶”ê°€
                        related_herbs = self.terms_manager.get_related_terms(
                            herb)
                        for related in related_herbs[:2]:  # ìµœëŒ€ 2ê°œë§Œ
                            if related in full_text:
                                keywords.append(related)
            except Exception as e:
                print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ì•½ì¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 3. ë™ì  TCM ìš©ì–´ ë§¤ì¹­
        for category, terms in self.dynamic_tcm_terms.items():
            for term in terms:
                if term in full_text and term not in keywords:
                    keywords.append(term)

        # 4. ì²˜ë°© ê´€ë ¨ íŠ¹ìˆ˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        prescription_keywords = self._extract_prescription_specific_keywords(
            full_text)
        keywords.extend(prescription_keywords)

        # 5. íš¨ëŠ¥/ì£¼ì¹˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        efficacy_keywords = self._extract_efficacy_keywords(full_text)
        keywords.extend(efficacy_keywords)

        return list(set(keywords))[:20]  # ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ

    def _extract_prescription_specific_keywords(self, text: str) -> List[str]:
        """ì²˜ë°© íŠ¹ìˆ˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []

        # ì²˜ë°© êµ¬ì„± ê´€ë ¨
        composition_patterns = [
            r'å³çˆ²æœ«', r'å³å‰‰', r'å³ä»¶', r'å³åŒ', r'å³å„', r'å³çˆ²ç´°æœ«',
            r'æ°´ç…æœ', r'é…’èª¿æœ', r'èœœä¸¸', r'æ¹¯èª¿æœ', r'ç©ºå¿ƒæœ'
        ]

        for pattern in composition_patterns:
            if re.search(pattern, text):
                keywords.append(pattern)

        # ì£¼ì¹˜ì¦ í‚¤ì›Œë“œ
        indication_keywords = ['ä¸»æ²»', 'æ²»', 'ç™‚', 'ä¸»', 'ç”¨æ–¼', 'é©ç”¨']
        for keyword in indication_keywords:
            if keyword in text:
                keywords.append(keyword)

        return keywords

    def _extract_efficacy_keywords(self, text: str) -> List[str]:
        """íš¨ëŠ¥ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []

        # íš¨ëŠ¥ ê´€ë ¨ í‚¤ì›Œë“œ
        efficacy_patterns = [
            r'è£œ([ä¸€-é¾¯]{1,2})', r'ç›Š([ä¸€-é¾¯]{1,2})', r'é¤Š([ä¸€-é¾¯]{1,2})',
            r'æ·¸([ä¸€-é¾¯]{1,2})', r'ç€‰([ä¸€-é¾¯]{1,2})', r'æº«([ä¸€-é¾¯]{1,2})',
            r'æ¶¼([ä¸€-é¾¯]{1,2})', r'æ•£([ä¸€-é¾¯]{1,2})', r'æ”¶([ä¸€-é¾¯]{1,2})'
        ]

        for pattern in efficacy_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                full_term = pattern.replace('([ä¸€-é¾¯]{1,2})', match)
                keywords.append(full_term)

        return keywords

    def finalize_chunk(self, chunk: Dict, context: Dict) -> Dict:
        """ì²­í¬ ìµœì¢…í™” (ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ)"""
        content_text = ' '.join(chunk['content'])
        normalized_text, simplified_text = self.normalize_text(content_text)
        keywords = self.extract_keywords_improved(normalized_text)

        return {
            'content': normalized_text,
            'content_simplified': simplified_text,
            'metadata': {
                **chunk['metadata'],
                **context,
                'keywords': keywords,
                'token_count': len(self.tokenizer.encode(normalized_text)),
                'char_count': len(normalized_text)
            }
        }

    def extract_keywords_improved(self, text: str) -> List[str]:
        """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜)"""
        keywords = []

        # 1. í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ì „ë¬¸ìš©ì–´ ë§¤ì¹­
        if self.terms_manager:
            try:
                # ì§ì ‘ ë§¤ì¹­
                for term in self.terms_manager.search_index.keys():
                    if len(term) >= 2 and term in text:
                        keywords.append(term)

                # ì¹´í…Œê³ ë¦¬ë³„ ìš°ì„ ìˆœìœ„ ë§¤ì¹­
                priority_categories = ['ì²˜ë°©', 'ì•½ë¬¼', 'ë³‘ì¦', 'ìƒë¦¬']
                for category in priority_categories:
                    category_terms = self.terms_manager.search_by_category(
                        category, limit=50)
                    for term_data in category_terms:
                        hanja = term_data.get('ìš©ì–´ëª…_í•œì', '')
                        hangul = term_data.get('ìš©ì–´ëª…', '')

                        if hanja and hanja in text and hanja not in keywords:
                            keywords.append(hanja)
                        if hangul and hangul in text and hangul not in keywords:
                            keywords.append(hangul)

            except Exception as e:
                print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # 2. ë™ì  TCM ìš©ì–´ ë§¤ì¹­
        for category, terms in self.dynamic_tcm_terms.items():
            for term in terms:
                if term in text and term not in keywords:
                    keywords.append(term)

        # 3. jieba ë¶„í•  ê²°ê³¼ ì¶”ê°€ (ì˜ë¯¸ìˆëŠ” ìš©ì–´ë§Œ)
        try:
            words = jieba.lcut(text)
            for word in words:
                if (len(word) >= 2 and
                    word not in keywords and
                        self._is_meaningful_medical_term(word)):
                    keywords.append(word)
        except Exception as e:
            print(f"âš ï¸ jieba ë¶„í•  ì‹¤íŒ¨: {e}")

        # 4. íŠ¹ìˆ˜ íŒ¨í„´ ë§¤ì¹­
        special_keywords = self._extract_special_pattern_keywords(text)
        keywords.extend(special_keywords)

        return list(set(keywords))[:15]  # ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì œí•œ

    def _is_meaningful_medical_term(self, word: str) -> bool:
        """ì˜ë¯¸ìˆëŠ” ì˜í•™ ìš©ì–´ì¸ì§€ íŒë‹¨"""
        if len(word) < 2:
            return False

        # í•œì ë¹„ìœ¨ í™•ì¸
        chinese_char_count = sum(
            1 for char in word if '\u4e00' <= char <= '\u9fff')
        if chinese_char_count / len(word) < 0.5:
            return False

        # ì˜í•™ ê´€ë ¨ íŠ¹ì§• í™•ì¸
        medical_indicators = [
            'ç—…', 'ç—‡', 'è­‰', 'ç—›', 'è™›', 'å¯¦', 'ç†±', 'å¯’', 'æ¿•', 'ç‡¥',
            'è£œ', 'ç›Š', 'é¤Š', 'æ·¸', 'ç€‰', 'æº«', 'æ¶¼', 'æ•£', 'æ”¶',
            'æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'é£²', 'ä¸¹', 'æ–¹'
        ]

        # ë™ì  íŒ¨í„´ê³¼ ë¹„êµ
        if any(indicator in word for indicator in medical_indicators):
            return True

        # í‘œì¤€ìš©ì–´ì§‘ì— ìˆëŠ”ì§€ í™•ì¸
        if self.terms_manager:
            try:
                return word in self.terms_manager.search_index
            except:
                pass

        return False

    def _extract_special_pattern_keywords(self, text: str) -> List[str]:
        """íŠ¹ìˆ˜ íŒ¨í„´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []

        # 1. ì²˜ë°© ê´€ë ¨ íŒ¨í„´
        for pattern in self.prescription_patterns:
            if pattern in text:
                keywords.append(pattern)

        # 2. ì•½ì¬ ê´€ë ¨ íŒ¨í„´
        herb_in_text = [herb for herb in self.herb_patterns if herb in text]
        keywords.extend(herb_in_text[:5])  # ìµœëŒ€ 5ê°œ

        # 3. ìˆ˜ëŸ‰ í‘œí˜„ê³¼ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ìš©ì–´
        quantity_patterns = [
            r'([ä¸€-é¾¯]{2,4})\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]\s*[éŒ¢å…©åˆ†æ–¤å‡åˆ]',
            r'([ä¸€-é¾¯]{2,4})\s*å„\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]',
            r'([ä¸€-é¾¯]{2,4})\s*(?:å°‘è¨±|é©é‡|è‹¥å¹²)'
        ]

        for pattern in quantity_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if len(match) >= 2 and self._is_meaningful_medical_term(match):
                    keywords.append(match)

        # 4. ë³‘ì¦ ê´€ë ¨ íŠ¹ìˆ˜ íŒ¨í„´
        symptom_patterns = [
            r'([ä¸€-é¾¯]{2,4}[è­‰ç—…ç—‡])',
            r'([ä¸€-é¾¯]{1,3}[è™›å¯¦])',
            r'([ä¸€-é¾¯]{2,4}[ç—›])',
            r'([ä¸€-é¾¯]{2,4}[ç†±å¯’])'
        ]

        for pattern in symptom_patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)

        return keywords

    def analyze_document_statistics(self, chunks: List[Dict]) -> Dict:
        """ë¬¸ì„œ í†µê³„ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)"""
        stats = {
            'total_chunks': len(chunks),
            'total_characters': 0,
            'total_tokens': 0,
            'content_types': defaultdict(int),
            'categories': defaultdict(int),
            'keyword_frequency': Counter(),
            'prescription_count': 0,
            'average_chunk_size': 0,
            'source_distribution': defaultdict(int)
        }

        for chunk in chunks:
            content = chunk['content']
            metadata = chunk['metadata']

            # ê¸°ë³¸ í†µê³„
            stats['total_characters'] += len(content)
            stats['total_tokens'] += metadata.get('token_count', 0)

            # ë‚´ìš© íƒ€ì… ë¶„ì„
            content_type = metadata.get('type', 'general')
            stats['content_types'][content_type] += 1

            # ì¹´í…Œê³ ë¦¬ ë¶„ì„
            bb_category = metadata.get('BB', 'unknown')
            cc_category = metadata.get('CC', 'unknown')
            stats['categories'][bb_category] += 1

            # ì²˜ë°© ê°œìˆ˜
            if content_type == 'prescription':
                stats['prescription_count'] += 1

            # í‚¤ì›Œë“œ ë¹ˆë„
            keywords = metadata.get('keywords', [])
            for keyword in keywords:
                stats['keyword_frequency'][keyword] += 1

            # ì¶œì²˜ ë¶„í¬
            source_file = metadata.get('source_file', 'unknown')
            stats['source_distribution'][source_file] += 1

        # í‰ê·  ì²­í¬ í¬ê¸°
        if stats['total_chunks'] > 0:
            stats['average_chunk_size'] = stats['total_characters'] / \
                stats['total_chunks']

        return stats

    def validate_chunks(self, chunks: List[Dict]) -> Dict:
        """ì²­í¬ ìœ íš¨ì„± ê²€ì¦"""
        validation_result = {
            'valid_chunks': 0,
            'invalid_chunks': 0,
            'warnings': [],
            'errors': [],
            'recommendations': []
        }

        for i, chunk in enumerate(chunks):
            try:
                content = chunk.get('content', '')
                metadata = chunk.get('metadata', {})

                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not content:
                    validation_result['errors'].append(f"ì²­í¬ {i}: ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
                    validation_result['invalid_chunks'] += 1
                    continue

                if not metadata:
                    validation_result['warnings'].append(f"ì²­í¬ {i}: ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŒ")

                # ë‚´ìš© ê¸¸ì´ ê²€ì¦
                if len(content) < 10:
                    validation_result['warnings'].append(
                        f"ì²­í¬ {i}: ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì)")
                elif len(content) > 2000:
                    validation_result['warnings'].append(
                        f"ì²­í¬ {i}: ë‚´ìš©ì´ ë„ˆë¬´ ê¹€ ({len(content)}ì)")

                # í‚¤ì›Œë“œ ê²€ì¦
                keywords = metadata.get('keywords', [])
                if not keywords:
                    validation_result['warnings'].append(f"ì²­í¬ {i}: í‚¤ì›Œë“œê°€ ì—†ìŒ")
                elif len(keywords) > 20:
                    validation_result['warnings'].append(
                        f"ì²­í¬ {i}: í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìŒ ({len(keywords)}ê°œ)")

                # ì²˜ë°© ì²­í¬ íŠ¹ë³„ ê²€ì¦
                if metadata.get('type') == 'prescription':
                    prescription_name = metadata.get('prescription_name')
                    if not prescription_name:
                        validation_result['errors'].append(
                            f"ì²­í¬ {i}: ì²˜ë°© ì²­í¬ì— ì²˜ë°©ëª…ì´ ì—†ìŒ")
                        validation_result['invalid_chunks'] += 1
                        continue

                validation_result['valid_chunks'] += 1

            except Exception as e:
                validation_result['errors'].append(f"ì²­í¬ {i}: ê²€ì¦ ì¤‘ ì˜¤ë¥˜ - {e}")
                validation_result['invalid_chunks'] += 1

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        total_chunks = len(chunks)
        if total_chunks > 0:
            error_rate = validation_result['invalid_chunks'] / total_chunks
            warning_rate = len(validation_result['warnings']) / total_chunks

            if error_rate > 0.05:
                validation_result['recommendations'].append(
                    "ì˜¤ë¥˜ìœ¨ì´ 5%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ë¬¸ì„œ íŒŒì‹± ë¡œì§ì„ ê²€í† í•˜ì„¸ìš”.")

            if warning_rate > 0.2:
                validation_result['recommendations'].append(
                    "ê²½ê³ ìœ¨ì´ 20%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì²­í‚¹ ì „ëµì„ ì¡°ì •í•˜ì„¸ìš”.")

        return validation_result

    def optimize_chunks_for_model(self, chunks: List[Dict], target_model: str = 'gpt-4o-mini') -> List[Dict]:
        """ëª¨ë¸ì— ìµœì í™”ëœ ì²­í¬ ì¡°ì •"""
        model_configs = {
            'gpt-4o-mini': {
                'max_tokens': 8192,
                'optimal_chunk_tokens': 500,
                'max_chunk_tokens': 1200
            },
            'gpt-4': {
                'max_tokens': 32768,
                'optimal_chunk_tokens': 800,
                'max_chunk_tokens': 2000
            }
        }

        config = model_configs.get(target_model, model_configs['gpt-4o-mini'])
        optimized_chunks = []

        for chunk in chunks:
            content = chunk['content']
            metadata = chunk['metadata']
            token_count = metadata.get(
                'token_count', len(self.tokenizer.encode(content)))

            if token_count <= config['max_chunk_tokens']:
                # ì ì • í¬ê¸° ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                optimized_chunks.append(chunk)
            else:
                # í° ì²­í¬ëŠ” ë¶„í• 
                split_chunks = self._split_large_chunk(
                    chunk, config['optimal_chunk_tokens'])
                optimized_chunks.extend(split_chunks)

        print(f"âœ… {target_model}ì— ìµœì í™”: {len(chunks)}ê°œ â†’ {len(optimized_chunks)}ê°œ ì²­í¬")
        return optimized_chunks

    def _split_large_chunk(self, chunk: Dict, target_tokens: int) -> List[Dict]:
        """í° ì²­í¬ ë¶„í• """
        content = chunk['content']
        metadata = chunk['metadata']

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ\n]', content)
        current_chunk_sentences = []
        current_tokens = 0
        split_chunks = []

        for sentence in sentences:
            if not sentence.strip():
                continue

            sentence_tokens = len(self.tokenizer.encode(sentence))

            if current_tokens + sentence_tokens > target_tokens and current_chunk_sentences:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_content = ''.join(current_chunk_sentences)
                normalized_text, simplified_text = self.normalize_text(
                    chunk_content)
                keywords = self.extract_keywords_improved(normalized_text)

                split_chunk = {
                    'content': normalized_text,
                    'content_simplified': simplified_text,
                    'metadata': {
                        **metadata,
                        'keywords': keywords,
                        'token_count': len(self.tokenizer.encode(normalized_text)),
                        'char_count': len(normalized_text),
                        'split_index': len(split_chunks)
                    }
                }
                split_chunks.append(split_chunk)

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk_sentences = [sentence]
                current_tokens = sentence_tokens
            else:
                current_chunk_sentences.append(sentence)
                current_tokens += sentence_tokens

        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk_sentences:
            chunk_content = ''.join(current_chunk_sentences)
            normalized_text, simplified_text = self.normalize_text(
                chunk_content)
            keywords = self.extract_keywords_improved(normalized_text)

            split_chunk = {
                'content': normalized_text,
                'content_simplified': simplified_text,
                'metadata': {
                    **metadata,
                    'keywords': keywords,
                    'token_count': len(self.tokenizer.encode(normalized_text)),
                    'char_count': len(normalized_text),
                    'split_index': len(split_chunks)
                }
            }
            split_chunks.append(split_chunk)

        return split_chunks

    def export_keywords_to_terms_manager(self, chunks: List[Dict]) -> Dict:
        """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ìš©ì–´ì§‘ ê´€ë¦¬ìë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not self.terms_manager:
            return {'status': 'error', 'message': 'ìš©ì–´ì§‘ ê´€ë¦¬ìê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}

        keyword_stats = Counter()
        new_terms = []

        for chunk in chunks:
            keywords = chunk['metadata'].get('keywords', [])
            for keyword in keywords:
                keyword_stats[keyword] += 1

        # ê³ ë¹ˆë„ í‚¤ì›Œë“œ ì¤‘ ìš©ì–´ì§‘ì— ì—†ëŠ” ê²ƒë“¤ ì°¾ê¸°
        for keyword, frequency in keyword_stats.most_common(100):
            if frequency >= 3 and len(keyword) >= 2:  # ìµœì†Œ 3íšŒ ì´ìƒ ë“±ì¥
                try:
                    if not self.terms_manager.get_term_info(keyword):
                        new_terms.append({
                            'term': keyword,
                            'frequency': frequency,
                            'category': self._infer_term_category(keyword)
                        })
                except:
                    continue

        return {
            'status': 'success',
            'total_keywords': len(keyword_stats),
            'new_terms_found': len(new_terms),
            'new_terms': new_terms[:20]  # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜
        }

    def _infer_term_category(self, term: str) -> str:
        """ìš©ì–´ ì¹´í…Œê³ ë¦¬ ì¶”ì •"""
        # íŒ¨í„´ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì •
        if any(suffix in term for suffix in ['æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'é£²']):
            return 'ì²˜ë°©'
        elif any(suffix in term for suffix in ['è­‰', 'ç—…', 'ç—‡']):
            return 'ë³‘ì¦'
        elif any(suffix in term for suffix in ['è™›', 'å¯¦', 'ç†±', 'å¯’']):
            return 'ë³‘ì¦'
        elif term in self.herb_patterns:
            return 'ì•½ë¬¼'
        elif any(concept in term for concept in ['é™°é™½', 'äº”è¡Œ', 'æ°£è¡€', 'ç¶“çµ¡']):
            return 'ì´ë¡ '
        else:
            return 'ê¸°íƒ€'

    def get_processing_statistics(self) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            'terms_manager_connected': self.terms_manager is not None,
            'dynamic_terms_count': sum(len(terms) for terms in self.dynamic_tcm_terms.values()),
            'prescription_patterns_count': len(self.prescription_patterns),
            'herb_patterns_count': len(self.herb_patterns),
            'fallback_mode': not bool(self.terms_manager),
            'jieba_words_added': True,
            'categories': list(self.dynamic_tcm_terms.keys())
        }

    def rebuild_dynamic_dictionary(self):
        """ë™ì  ì‚¬ì „ ì¬êµ¬ì¶•"""
        print("ğŸ”„ ë™ì  TCM ì‚¬ì „ ì¬êµ¬ì¶• ì¤‘...")
        self.dynamic_tcm_terms.clear()
        self.prescription_patterns.clear()
        self.herb_patterns.clear()

        self._build_dynamic_tcm_dictionary()
        print("âœ… ë™ì  TCM ì‚¬ì „ ì¬êµ¬ì¶• ì™„ë£Œ")

    def clear_jieba_cache(self):
        """jieba ìºì‹œ ì´ˆê¸°í™”"""
        try:
            jieba.dt.cache_file = None
            print("âœ… jieba ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ jieba ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def update_terms_manager(self, new_terms_manager):
        """ìš©ì–´ì§‘ ê´€ë¦¬ì ì—…ë°ì´íŠ¸"""
        old_manager = self.terms_manager
        self.terms_manager = new_terms_manager

        if new_terms_manager != old_manager:
            print("ğŸ”„ ìƒˆë¡œìš´ ìš©ì–´ì§‘ ê´€ë¦¬ìë¡œ ì—…ë°ì´íŠ¸ ì¤‘...")
            self.setup_chinese_tools()
            self._build_dynamic_tcm_dictionary()
            print("âœ… ìš©ì–´ì§‘ ê´€ë¦¬ì ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    def validate_terms_manager_connection(self) -> bool:
        """ìš©ì–´ì§‘ ê´€ë¦¬ì ì—°ê²° ìœ íš¨ì„± ê²€ì¦"""
        if not self.terms_manager:
            return False

        try:
            # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            test_result = self.terms_manager.get_term_info('è¡€è™›')
            return test_result is not None
        except Exception as e:
            print(f"âš ï¸ ìš©ì–´ì§‘ ê´€ë¦¬ì ì—°ê²° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def get_recommended_settings(self) -> Dict:
        """ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
        return {
            'chunk_strategy': 'hierarchical',
            'max_chunk_tokens': 1200,
            'optimal_chunk_tokens': 500,
            'enable_terms_manager': True,
            'extract_prescriptions': True,
            'extract_detailed_keywords': True,
            'normalize_traditional_chinese': True,
            'split_large_chunks': True,
            'validate_chunks': True
        }

# í¸ì˜ í•¨ìˆ˜ë“¤


def create_document_processor(data_path: str, terms_manager=None) -> DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ê¸° ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return DocumentProcessor(data_path=data_path, terms_manager=terms_manager)


def test_document_processor():
    """í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜"""
    print("ğŸ§ª ë¬¸ì„œ ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸")

    # í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œ (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    test_data_path = "/Users/radi/Projects/langchainDATA/RAWDATA/DYBG"

    processor = DocumentProcessor(data_path=test_data_path)

    # ì²˜ë¦¬ í†µê³„ ì¶œë ¥
    stats = processor.get_processing_statistics()
    print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # ë™ì  ìš©ì–´ ì‚¬ì „ ì •ë³´
    print(f"\nğŸ“š ë™ì  TCM ìš©ì–´ ì‚¬ì „:")
    for category, terms in processor.dynamic_tcm_terms.items():
        print(f"   {category}: {len(terms)}ê°œ ìš©ì–´")
        if terms:
            print(f"      ì˜ˆì‹œ: {', '.join(terms[:5])}")


if __name__ == "__main__":
    test_document_processor()
