#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ì˜ë³´ê° ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ - document_processor.py
ë¬¸ì„œ ë¡œë”©, íŒŒì‹±, ì²­í‚¹ì„ ë‹´ë‹¹
"""

import os
import re
import unicodedata
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings("ignore")

try:
    import jieba
    import opencc
    import tiktoken
except ImportError as e:
    print(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    raise


class DocumentProcessor:
    def __init__(self, data_path: str, terms_manager=None):
        """ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        self.data_path = Path(data_path)
        self.terms_manager = terms_manager

        # ì¤‘êµ­ì–´ ì²˜ë¦¬ ë„êµ¬ ì´ˆê¸°í™”
        self.cc = opencc.OpenCC('t2s')
        self.setup_chinese_tools()

        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (GPT-4 í˜¸í™˜)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # ì¤‘ì˜í•™ ì „ë¬¸ ìš©ì–´ ì‚¬ì „ (ê¸°ë³¸ ë°±ì—…ìš©)
        self.tcm_terms = self.load_basic_tcm_terms()

    def setup_chinese_tools(self):
        """ì¤‘êµ­ì–´ ì²˜ë¦¬ ë„êµ¬ ì„¤ì •"""
        try:
            if self.terms_manager and hasattr(self.terms_manager, 'search_index'):
                print("ğŸ“š ì¤‘ì˜í•™ ì „ë¬¸ìš©ì–´ë¥¼ jiebaì— ì¶”ê°€ ì¤‘...")
                added_count = 0

                for term in self.terms_manager.search_index.keys():
                    if len(term) >= 2 and self._is_chinese_term(term):
                        jieba.add_word(term)
                        added_count += 1

                print(f"âœ… {added_count}ê°œ ì „ë¬¸ìš©ì–´ ì¶”ê°€ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._setup_basic_jieba_terms()

    def _is_chinese_term(self, term: str) -> bool:
        """í•œì ìš©ì–´ì¸ì§€ í™•ì¸"""
        return all('\u4e00' <= char <= '\u9fff' for char in term)

    def _setup_basic_jieba_terms(self):
        """ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ë¥¼ jiebaì— ì¶”ê°€"""
        basic_terms = [
            "äººåƒ", "ç•¶æ­¸", "å·èŠ", "ç™½èŠ", "ç†Ÿåœ°é»ƒ", "ç”Ÿåœ°é»ƒ",
            "é©šæ‚¸", "å¥å¿˜", "ç™²ç™‡", "çœ©æšˆ", "å¤±çœ ", "è™›å‹",
            "å››ç‰©æ¹¯", "å…­å›å­æ¹¯", "è£œä¸­ç›Šæ°£æ¹¯", "çŠ€è§’åœ°é»ƒæ¹¯",
            "é™°é™½", "äº”è¡Œ", "è‡Ÿè…‘", "æ°£è¡€", "ç¶“çµ¡", "ç²¾æ°£ç¥",
            "è¡€è™›", "æ°£è™›", "é™°è™›", "é™½è™›", "è„¾èƒƒ", "è‚è…"
        ]
        for term in basic_terms:
            jieba.add_word(term)
        print(f"âœ… {len(basic_terms)}ê°œ ê¸°ë³¸ ì¤‘ì˜í•™ ìš©ì–´ ì¶”ê°€ ì™„ë£Œ")

    def load_basic_tcm_terms(self) -> Dict[str, List[str]]:
        """ê¸°ë³¸ ì¤‘ì˜í•™ ì „ë¬¸ ìš©ì–´ ì‚¬ì „ ë¡œë“œ"""
        return {
            "symptoms": ["é©šæ‚¸", "å¥å¿˜", "ç™²ç™‡", "çœ©æšˆ", "å¤±çœ ", "è™›å‹", "è¡€è™›", "æ°£è™›"],
            "herbs": ["äººåƒ", "ç•¶æ­¸", "å·èŠ", "ç™½èŠ", "ç†Ÿåœ°é»ƒ", "ç”Ÿåœ°é»ƒ", "é»ƒèŠª", "èŒ¯è‹“"],
            "prescriptions": ["å››ç‰©æ¹¯", "å…­å›å­æ¹¯", "è£œä¸­ç›Šæ°£æ¹¯", "çŠ€è§’åœ°é»ƒæ¹¯", "ç•¶æ­¸è£œè¡€æ¹¯"],
            "theories": ["é™°é™½", "äº”è¡Œ", "è‡Ÿè…‘", "æ°£è¡€", "ç¶“çµ¡", "ç²¾æ°£ç¥", "å›è‡£ä½ä½¿"]
        }

    def normalize_text(self, text: str) -> Tuple[str, str]:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ë²ˆì²´ ìœ ì§€ + ê°„ì²´ ë³€í™˜)"""
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        normalized = unicodedata.normalize('NFKC', text)
        # ê³µë°± ì •ë¦¬
        normalized = re.sub(r'[ã€€\s]+', ' ', normalized)
        # ê°„ì²´ ë³€í™˜ (ê²€ìƒ‰ í™•ì¥ìš©)
        simplified = self.cc.convert(normalized)
        return normalized, simplified

    def calculate_data_hash(self) -> str:
        """ë°ì´í„° ë³€ê²½ ê°ì§€ìš© í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        file_info = []

        for root, dirs, files in os.walk(self.data_path):
            for file in sorted(files):
                if file.endswith('.txt'):
                    file_path = Path(root) / file
                    stat = file_path.stat()
                    file_info.append(f"{file}:{stat.st_mtime}:{stat.st_size}")

        hash_md5.update('|'.join(file_info).encode('utf-8'))
        return hash_md5.hexdigest()

    def load_documents(self) -> List[Dict]:
        """ë™ì˜ë³´ê° ë¬¸ì„œ ë¡œë“œ"""
        print("ğŸ“š ë™ì˜ë³´ê° ë¬¸ì„œ ë¡œë”© ì¤‘...")
        all_chunks = []

        for root, dirs, files in os.walk(self.data_path):
            for file in files:
                if file.endswith('.txt'):
                    file_path = Path(root) / file
                    print(f"   ğŸ“„ {file_path.name} ì²˜ë¦¬ ì¤‘...")

                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        chunks = self.parse_document_structure(content)
                        for chunk in chunks:
                            chunk['metadata']['source_file'] = file_path.name
                            chunk['metadata']['source_path'] = str(file_path)

                        all_chunks.extend(chunks)

                    except Exception as e:
                        print(f"âš ï¸ {file_path.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks

    def parse_document_structure(self, text: str) -> List[Dict]:
        """ë™ì˜ë³´ê° ë¬¸ì„œ êµ¬ì¡° íŒŒì‹±"""
        lines = text.split('\n')
        chunks = []
        current_context = {
            'AA': '',  # í¸ëª…/ê¶Œëª…
            'XX': '',  # ì €ì ì •ë³´
            'BB': '',  # ëŒ€ë¶„ë¥˜
            'CC': '',  # ì¤‘ë¶„ë¥˜
            'DD': '',  # ì†Œë¶„ë¥˜
        }

        current_chunk = {'content': [], 'metadata': {}}
        current_prescription = None
        prescription_content = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # êµ¬ì¡° ë§ˆì»¤ í™•ì¸
            if len(line) > 2 and line[:2] in ['AA', 'XX', 'OO', 'ZZ', 'BB', 'CC', 'DD', 'DP', 'SS', 'PP']:
                marker = line[:2]
                content = line[2:].strip()

                # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                if marker in current_context:
                    current_context[marker] = content

                # ì²­í‚¹ ë¡œì§
                if marker == 'BB':  # ìƒˆë¡œìš´ ëŒ€ë¶„ë¥˜
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                        current_prescription = None
                        prescription_content = []

                    if current_chunk['content']:
                        chunks.append(self.finalize_chunk(
                            current_chunk, current_context))
                    current_chunk = {'content': [
                        content], 'metadata': dict(current_context)}

                elif marker == 'CC':  # ìƒˆë¡œìš´ ì¤‘ë¶„ë¥˜
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                        current_prescription = None
                        prescription_content = []

                    if current_chunk['content'] and len(' '.join(current_chunk['content'])) > 50:
                        chunks.append(self.finalize_chunk(
                            current_chunk, current_context))
                    current_chunk = {'content': [
                        content], 'metadata': dict(current_context)}

                elif marker == 'DP':  # ì²˜ë°© ì‹œì‘
                    if current_prescription:
                        chunks.append(self.create_prescription_chunk(
                            current_prescription, prescription_content, current_context))
                    current_prescription = content
                    prescription_content = []

                elif marker == 'SS':  # ì²˜ë°© ìƒì„¸
                    if current_prescription:
                        prescription_content.append(content)

                elif marker == 'ZZ':  # ë³¸ë¬¸ ë‚´ìš©
                    if not current_prescription:
                        current_chunk['content'].append(content)
                    else:
                        prescription_content.append(content)

        # ë§ˆì§€ë§‰ ì²˜ë¦¬
        if current_prescription:
            chunks.append(self.create_prescription_chunk(
                current_prescription, prescription_content, current_context))
        if current_chunk['content']:
            chunks.append(self.finalize_chunk(current_chunk, current_context))

        return chunks

    def create_prescription_chunk(self, prescription_name: str, prescription_content: List[str], context: Dict) -> Dict:
        """ì²˜ë°© ì „ìš© ì²­í¬ ìƒì„±"""
        full_content = f"è™•æ–¹: {prescription_name}\n\n" + \
            '\n'.join(prescription_content)
        normalized_text, simplified_text = self.normalize_text(full_content)
        keywords = self.extract_prescription_keywords(
            prescription_name, prescription_content)

        return {
            'content': normalized_text,
            'content_simplified': simplified_text,
            'metadata': {
                **context,
                'type': 'prescription',
                'prescription_name': prescription_name,
                'keywords': keywords,
                'token_count': len(self.tokenizer.encode(normalized_text)),
                'char_count': len(normalized_text)
            }
        }

    def extract_prescription_keywords(self, prescription_name: str, content: List[str]) -> List[str]:
        """ì²˜ë°© ì „ìš© í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = [prescription_name]
        full_text = ' '.join(content)

        # ì•½ì¬ëª… íŒ¨í„´ ë§¤ì¹­
        herb_patterns = [
            r'([ä¸€-é¾¯]{2,4})\s*(?:å„|æ¯|ç”¨|å–)',
            r'([ä¸€-é¾¯]{2,4})\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒë§Œ]\s*[éŒ¢å…©åˆ†æ–¤]',
        ]

        for pattern in herb_patterns:
            matches = re.findall(pattern, full_text)
            keywords.extend(matches)

        # ì¦ìƒ í‚¤ì›Œë“œ ì¶”ì¶œ
        for term_list in self.tcm_terms.values():
            for term in term_list:
                if term in full_text:
                    keywords.append(term)

        return list(set(keywords))[:15]

    def finalize_chunk(self, chunk: Dict, context: Dict) -> Dict:
        """ì²­í¬ ìµœì¢…í™”"""
        content_text = ' '.join(chunk['content'])
        normalized_text, simplified_text = self.normalize_text(content_text)
        keywords = self.extract_keywords(normalized_text)

        return {
            'content': normalized_text,
            'content_simplified': simplified_text,
            'metadata': {
                **chunk['metadata'],
                **context,
                'keywords': keywords,
                'token_count': len(self.tokenizer.encode(normalized_text)),
                'char_count': len(normalized_text)
            }
        }

    def extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []

        # ì¤‘ì˜í•™ ì „ë¬¸ìš©ì–´ ë§¤ì¹­
        for category, terms in self.tcm_terms.items():
            for term in terms:
                if term in text:
                    keywords.append(term)

        # jieba ë¶„í•  ê²°ê³¼ ì¶”ê°€
        words = jieba.lcut(text)
        for word in words:
            if len(word) >= 2 and word not in keywords:
                keywords.append(word)

        return list(set(keywords))[:10]
