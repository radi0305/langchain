#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM ê´€ë¦¬ì - OpenAI ì „ìš© ë²„ì „ (gpt-4o-mini)
dongui_rag_system.pyì—ì„œ importí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë…ë¦½ ëª¨ë“ˆ
"""

import os
from typing import List, Dict, Optional, Any
import warnings
warnings.filterwarnings("ignore")

try:
    from dotenv import load_dotenv
    from langchain_community.chat_models import ChatOpenAI
    from langchain.schema import BaseMessage, SystemMessage, HumanMessage
    import tiktoken
except ImportError as e:
    print(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    print("pip install langchain-community openai python-dotenv tiktoken")
    raise


class LLMManager:
    """OpenAI LLM ì—°ê²° ë° ê´€ë¦¬ í´ë˜ìŠ¤ (gpt-4o-mini ì „ìš©)"""

    def __init__(self):
        """OpenAI LLM ê´€ë¦¬ì ì´ˆê¸°í™”"""
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()

        # ëª¨ë¸ ì„¤ì •
        self.model_config = {
            'model_name': 'gpt-4o-mini',
            'max_context_tokens': 128000,
            'max_response_tokens': 4000,
            'safe_context_tokens': 120000,  # ì•ˆì „ ì—¬ìœ ë¶„ í™•ë³´
            'optimal_k': 75,  # ìµœì  Kê°’
            'timeout': 60
        }

        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # OpenAI ì´ˆê¸°í™”
        self.llm = None
        self._available = False
        self._setup_openai()

    def _setup_openai(self):
        """OpenAI GPT-4o-mini ì„¤ì •"""
        try:
            # API í‚¤ í™•ì¸
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
                print("   OPENAI_API_KEY=your-api-key-here")
                self.llm = None
                return False

            # GPT-4o-mini ì´ˆê¸°í™”
            self.llm = ChatOpenAI(
                model=self.model_config['model_name'],
                temperature=0,
                timeout=self.model_config['timeout'],
                max_tokens=self.model_config['max_response_tokens'],
                openai_api_key=api_key
            )

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            print("ğŸ”— OpenAI GPT-4o-mini ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
            print("âœ… OpenAI GPT-4o-mini ì—°ê²° ì„±ê³µ!")
            print(f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {self.model_config['max_context_tokens']:,} í† í°")
            print(f"ğŸ¯ ìµœì  Kê°’: {self.model_config['optimal_k']}ê°œ")
            self._available = True
            return True

        except Exception as e:
            print(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
            print("API í‚¤ë¥¼ í™•ì¸í•˜ê³  ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            self.llm = None
            self._available = False
            return False

    def is_available(self) -> bool:
        """LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self._available and self.llm is not None

    def get_model_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if not self.is_available():
            return {
                'type': 'none',
                'name': 'unavailable',
                'display_name': 'ì‚¬ìš© ë¶ˆê°€',
                'max_context_tokens': 0,
                'safe_context_tokens': 0,
                'optimal_k': 0,
                'is_connected': False
            }

        return {
            'type': 'openai',
            'name': self.model_config['model_name'],
            'display_name': 'OpenAI GPT-4o-mini',
            'max_context_tokens': self.model_config['max_context_tokens'],
            'safe_context_tokens': self.model_config['safe_context_tokens'],
            'optimal_k': self.model_config['optimal_k'],
            'is_connected': True
        }

    def get_optimal_k_value(self) -> int:
        """ìµœì  Kê°’ ë°˜í™˜"""
        return self.model_config['optimal_k']

    def get_safe_context_tokens(self) -> int:
        """ì•ˆì „í•œ ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ë°˜í™˜"""
        return self.model_config['safe_context_tokens']

    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        if not self.tokenizer:
            # ëŒ€ëµì ì¸ ì¶”ì • (í•œê¸€/í•œì ê³ ë ¤)
            return len(text) // 2

        try:
            return len(self.tokenizer.encode(text))
        except Exception:
            return len(text) // 2

    def optimize_context_for_model(self, context_parts: List[str]) -> List[str]:
        """GPT-4o-miniì— ë§ê²Œ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"""
        if not context_parts:
            return []

        safe_tokens = self.get_safe_context_tokens()
        optimized_parts = []
        total_tokens = 0

        # ì˜ˆì•½ í† í° (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ì§ˆë¬¸ + ì‘ë‹µìš©)
        reserved_tokens = 8000
        available_tokens = safe_tokens - reserved_tokens

        for part in context_parts:
            part_tokens = self.count_tokens(part)

            if total_tokens + part_tokens <= available_tokens:
                optimized_parts.append(part)
                total_tokens += part_tokens
            else:
                # ë‚¨ì€ í† í°ìœ¼ë¡œ ë¶€ë¶„ í¬í•¨ ì‹œë„
                remaining_tokens = available_tokens - total_tokens
                if remaining_tokens > 500:  # ìµœì†Œ í† í° ìˆ˜ í™•ë³´
                    truncated = self._truncate_text_safely(
                        part, remaining_tokens)
                    if truncated:
                        optimized_parts.append(truncated)
                break

        print(
            f"ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ìµœì í™”: {len(context_parts)}ê°œ â†’ {len(optimized_parts)}ê°œ ë¬¸ì„œ")
        print(f"ğŸ”¢ ì´ í† í° ìˆ˜: ì•½ {total_tokens:,}ê°œ (ì œí•œ: {available_tokens:,}ê°œ)")

        return optimized_parts

    def _truncate_text_safely(self, text: str, max_tokens: int) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°"""
        if self.count_tokens(text) <= max_tokens:
            return text

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
        sentences = text.split('.')
        truncated = ""

        for sentence in sentences:
            test_text = truncated + sentence + "."
            if self.count_tokens(test_text) <= max_tokens:
                truncated = test_text
            else:
                break

        if not truncated:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œë„ ì•ˆë˜ë©´ ë¬¸ì ë‹¨ìœ„ë¡œ
            estimated_chars = max_tokens * 2  # ëŒ€ëµì ì¸ ì¶”ì •
            truncated = text[:estimated_chars] + "..."

        return truncated

    def generate_response(self, messages: List[BaseMessage]) -> str:
        """LLMìœ¼ë¡œë¶€í„° ì‘ë‹µ ìƒì„±"""
        if not self.is_available():
            return "OpenAI GPT-4o-miniê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        try:
            # ë©”ì‹œì§€ í† í° ìˆ˜ í™•ì¸
            total_tokens = sum(self.count_tokens(msg.content)
                               for msg in messages)
            print(f"ğŸ”¢ ìš”ì²­ í† í° ìˆ˜: ì•½ {total_tokens:,}ê°œ")

            if total_tokens > self.model_config['safe_context_tokens']:
                print("âš ï¸ í† í° ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ì‘ë‹µì´ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # OpenAI API í˜¸ì¶œ
            response = self.llm(messages)

            if not response or not response.content:
                return "OpenAIë¡œë¶€í„° ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."

            return response.content

        except Exception as e:
            error_msg = str(e).lower()

            if 'rate limit' in error_msg:
                return "OpenAI API ìš”ì²­ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif 'api key' in error_msg or 'authentication' in error_msg:
                return "OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            elif 'context length' in error_msg or 'token' in error_msg:
                return "ìš”ì²­í•œ ë‚´ìš©ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                return f"OpenAI API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n\nê²€ìƒ‰ëœ ì›ë¬¸ì„ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”."

    def print_model_status(self):
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥"""
        model_info = self.get_model_info()
        status = "âœ… ì—°ê²°ë¨" if model_info['is_connected'] else "âŒ ì—°ê²° ì•ˆë¨"

        print(f"ğŸ¤– í˜„ì¬ ëª¨ë¸: {model_info['display_name']} ({status})")
        print(f"ğŸ“Š ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸: {model_info['max_context_tokens']:,} í† í°")
        print(f"ğŸ¯ ê¶Œì¥ Kê°’: {model_info['optimal_k']}ê°œ")


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_llm_manager() -> LLMManager:
    """LLM ê´€ë¦¬ì ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return LLMManager()


def setup_openai_api_key():
    """OpenAI API í‚¤ ì„¤ì • ë„ìš°ë¯¸"""
    print("\nğŸ”‘ OpenAI API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    print("1. https://platform.openai.com/api-keys ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”.")
    print("2. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
    print()
    print("ë°©ë²• 1: .env íŒŒì¼ ìƒì„± (ê¶Œì¥)")
    print("   í”„ë¡œì íŠ¸ í´ë”ì— .env íŒŒì¼ì„ ë§Œë“¤ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€:")
    print("   OPENAI_API_KEY=your-api-key-here")
    print()
    print("ë°©ë²• 2: í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
    print("   í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:")
    print("   export OPENAI_API_KEY=your-api-key-here")
    print()

    api_key = input("API í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Enterë¡œ ê±´ë„ˆë›°ê¸°): ").strip()

    if api_key:
        # ì„ì‹œë¡œ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •
        os.environ['OPENAI_API_KEY'] = api_key
        print("âœ… API í‚¤ê°€ ì„ì‹œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True

    return False


def test_llm_connection() -> bool:
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    manager = LLMManager()
    return manager.is_available()


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§ª OpenAI LLM ê´€ë¦¬ì í…ŒìŠ¤íŠ¸")

    llm_manager = LLMManager()

    if not llm_manager.is_available():
        if setup_openai_api_key():
            llm_manager = LLMManager()

        if not llm_manager.is_available():
            print("âŒ OpenAI ì„¤ì •ì„ ì™„ë£Œí•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return

    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    llm_manager.print_model_status()

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    try:
        test_messages = [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ë¡œ ë‹µí•´ì£¼ì„¸ìš”.")]
        response = llm_manager.generate_response(test_messages)
        print(f"ğŸ¤– í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()
