#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ì˜ë³´ê° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ - search_engine_improved.py (ê°œì„ ëœ ë²„ì „)
í•˜ë“œì½”ë”©ëœ ë¶€ë¶„ì„ í‘œì¤€í•œì˜í•™ìš©ì–´ì§‘ ê¸°ë°˜ìœ¼ë¡œ êµì²´
ì„ë² ë”©, ì¸ë±ì‹±, ê²€ìƒ‰ ë¡œì§ì„ ë‹´ë‹¹
"""

import numpy as np
import faiss
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
from collections import defaultdict
import warnings
warnings.filterwarnings("ignore")


class SearchEngine:
    def __init__(self, embedding_model_name: str = 'paraphrase-multilingual-mpnet-base-v2'):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        self.chunks = []
        self.embeddings = None
        self.faiss_index = None
        self.terms_manager = None

        # ë™ì  íŒ¨í„´ ìºì‹œ
        self._pattern_cache = {}
        self._relation_cache = {}

    def set_terms_manager(self, terms_manager):
        """í‘œì¤€ìš©ì–´ì§‘ ê´€ë¦¬ì ì„¤ì •"""
        self.terms_manager = terms_manager
        if terms_manager:
            self._build_dynamic_patterns()
            print("âœ… í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ë™ì  íŒ¨í„´ êµ¬ì¶• ì™„ë£Œ")

    def _build_dynamic_patterns(self):
        """í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ë™ì  íŒ¨í„´ êµ¬ì¶•"""
        if not self.terms_manager:
            return

        try:
            # ì²˜ë°© íŒ¨í„´ ë™ì  ìƒì„±
            prescriptions = self.terms_manager.search_by_category(
                'ì²˜ë°©', limit=200)
            prescription_suffixes = set()

            for prescription in prescriptions:
                hanja = prescription.get('ìš©ì–´ëª…_í•œì', '')
                if hanja:
                    for suffix in ['æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'é£²', 'ä¸¹', 'æ–¹']:
                        if hanja.endswith(suffix):
                            prescription_suffixes.add(suffix)

            self._pattern_cache['prescription_suffixes'] = list(
                prescription_suffixes)

            # ë³‘ì¦ íŒ¨í„´ ë™ì  ìƒì„±
            symptoms = self.terms_manager.search_by_category('ë³‘ì¦', limit=200)
            symptom_suffixes = set()

            for symptom in symptoms:
                hanja = symptom.get('ìš©ì–´ëª…_í•œì', '')
                if hanja:
                    for suffix in ['è­‰', 'ç—…', 'ç—‡', 'ç—›', 'è™›', 'å¯¦', 'å¯’', 'ç†±', 'æ¿•', 'ç‡¥']:
                        if hanja.endswith(suffix):
                            symptom_suffixes.add(suffix)

            self._pattern_cache['symptom_suffixes'] = list(symptom_suffixes)

            # ì•½ë¬¼ ë¦¬ìŠ¤íŠ¸ ë™ì  ìƒì„±
            herbs = self.terms_manager.search_by_category('ì•½ë¬¼', limit=100)
            herb_list = []

            for herb in herbs:
                hanja = herb.get('ìš©ì–´ëª…_í•œì', '')
                if hanja and len(hanja) >= 2:
                    herb_list.append(hanja)

            self._pattern_cache['major_herbs'] = herb_list

            # ì´ë¡  ê°œë… ë™ì  ìƒì„±
            theories = self.terms_manager.search_by_category('ìƒë¦¬', limit=50)
            theories.extend(
                self.terms_manager.search_by_category('ë³‘ë¦¬', limit=50))

            theory_concepts = []
            for theory in theories:
                hanja = theory.get('ìš©ì–´ëª…_í•œì', '')
                if hanja and len(hanja) >= 2:
                    theory_concepts.append(hanja)

            self._pattern_cache['theory_concepts'] = theory_concepts

            print(f"ğŸ“Š ë™ì  íŒ¨í„´ êµ¬ì¶• ì™„ë£Œ: ì²˜ë°© íŒ¨í„´ {len(prescription_suffixes)}ê°œ, "
                  f"ë³‘ì¦ íŒ¨í„´ {len(symptom_suffixes)}ê°œ, ì•½ë¬¼ {len(herb_list)}ê°œ, ì´ë¡  {len(theory_concepts)}ê°œ")

        except Exception as e:
            print(f"âš ï¸ ë™ì  íŒ¨í„´ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self._fallback_patterns()

    def _fallback_patterns(self):
        """í´ë°± íŒ¨í„´ (í‘œì¤€ìš©ì–´ì§‘ ì‹¤íŒ¨ì‹œ)"""
        self._pattern_cache = {
            'prescription_suffixes': ['æ¹¯', 'æ•£', 'ä¸¸', 'è†'],
            'symptom_suffixes': ['è­‰', 'ç—…', 'ç—‡', 'ç—›', 'è™›', 'å¯¦'],
            'major_herbs': ['äººåƒ', 'ç•¶æ­¸', 'å·èŠ', 'ç™½èŠ', 'ç†Ÿåœ°é»ƒ', 'é»ƒèŠª', 'ç™½æœ®', 'èŒ¯è‹“', 'ç”˜è‰'],
            'theory_concepts': ['é™°é™½', 'äº”è¡Œ', 'è‡Ÿè…‘', 'æ°£è¡€', 'ç¶“çµ¡', 'ç²¾æ°£ç¥']
        }

    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:
        """ì„ë² ë”© ìƒì„±"""
        print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")

        texts = []
        for chunk in chunks:
            title_parts = []
            if chunk['metadata'].get('BB'):
                title_parts.append(chunk['metadata']['BB'])
            if chunk['metadata'].get('CC'):
                title_parts.append(chunk['metadata']['CC'])

            title = ' '.join(title_parts)
            combined_text = f"{title} {chunk['content']}" if title else chunk['content']
            texts.append(combined_text)

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        batch_size = 32
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(
                batch, show_progress_bar=True, batch_size=batch_size)
            embeddings.append(batch_embeddings)

        embeddings = np.vstack(embeddings)
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        return embeddings

    def build_faiss_index(self, embeddings: np.ndarray):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ” FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)

        # ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        faiss.normalize_L2(embeddings)
        index.add(embeddings.astype('float32'))

        print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index

    def setup(self, chunks: List[Dict], embeddings: np.ndarray = None):
        """ê²€ìƒ‰ ì—”ì§„ ì„¤ì •"""
        self.chunks = chunks

        if embeddings is None:
            self.embeddings = self.create_embeddings(chunks)
        else:
            self.embeddings = embeddings

        self.faiss_index = self.build_faiss_index(self.embeddings)

    def search(self, query: str, k: int = 75) -> List[Dict]:
        """ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜"""
        if self.faiss_index is None:
            raise ValueError("ê²€ìƒ‰ ì—”ì§„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print(f"ğŸ” '{query}' ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ ì¤‘...")

        # 1ë‹¨ê³„: ëŒ€ëŸ‰ í›„ë³´ ìˆ˜ì§‘
        all_candidates = self._collect_comprehensive_candidates(query, k)

        # 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        clustered_candidates = self._cluster_by_content_type(
            query, all_candidates)

        # 3ë‹¨ê³„: ëŒ€í‘œ ì„ ì •
        final_results = self._select_representatives(clustered_candidates, k)

        print(f"âœ… ìµœì¢… {len(final_results)}ê°œ ëŒ€í‘œ ê²°ê³¼ ì„ ì •")
        return final_results

    def _semantic_search(self, query: str, k: int) -> List[Dict]:
        """ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.faiss_index.search(
            query_embedding.astype('float32'), k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append({
                    'content': chunk['content'],
                    'metadata': chunk['metadata'],
                    'semantic_score': float(score),
                    'chunk_index': idx
                })

        return results

    def _enhanced_keyword_search(self, query: str, k: int) -> List[Dict]:
        """ê°•í™”ëœ í‚¤ì›Œë“œ ê²€ìƒ‰ (ìš©ì–´ì§‘ ê¸°ë°˜)"""
        keyword_results = []

        for i, chunk in enumerate(self.chunks):
            content = chunk['content']
            score = 0.0

            # ì™„ì „ ë§¤ì¹­
            if query in content:
                score += 5.0

            # ë¶€ë¶„ ë§¤ì¹­
            for char in query:
                if char in content:
                    score += 0.2

            # ì²˜ë°©ëª… ë§¤ì¹­ (ë™ì  íŒ¨í„´ ì‚¬ìš©)
            if chunk['metadata'].get('prescription_name'):
                prescription_name = chunk['metadata']['prescription_name']
                if query in prescription_name or prescription_name in query:
                    score += 8.0

            # ë™ì  íŒ¨í„´ ë§¤ì¹­
            pattern_score = self._calculate_pattern_matching_score(
                query, content, chunk['metadata'])
            score += pattern_score

            # í‚¤ì›Œë“œ ë§¤ì¹­
            keywords = chunk['metadata'].get('keywords', [])
            for keyword in keywords:
                if query in keyword or keyword in query:
                    score += 1.0

            # ìœ„ì¹˜ ê°€ì¤‘ì¹˜
            if query in content:
                position = content.find(query)
                position_weight = max(0, 1.0 - (position / len(content)))
                score += position_weight * 2.0

            if score > 1.0:
                keyword_results.append({
                    'content': content,
                    'metadata': chunk['metadata'],
                    'semantic_score': score,
                    'chunk_index': i
                })

        keyword_results.sort(key=lambda x: x['semantic_score'], reverse=True)
        return keyword_results[:k]

    def _calculate_pattern_matching_score(self, query: str, content: str, metadata: Dict) -> float:
        """ë™ì  íŒ¨í„´ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # ì²˜ë°© íŒ¨í„´ ë§¤ì¹­
        prescription_suffixes = self._pattern_cache.get(
            'prescription_suffixes', [])
        for suffix in prescription_suffixes:
            if query.endswith(suffix) and suffix in content:
                score += 2.0
                break

        # ë³‘ì¦ íŒ¨í„´ ë§¤ì¹­
        symptom_suffixes = self._pattern_cache.get('symptom_suffixes', [])
        for suffix in symptom_suffixes:
            if query.endswith(suffix) and suffix in content:
                score += 1.5
                break

        # ì•½ë¬¼ ë§¤ì¹­
        major_herbs = self._pattern_cache.get('major_herbs', [])
        if query in major_herbs:
            for herb in major_herbs:
                if herb in content:
                    score += 1.0

        # ì´ë¡  ê°œë… ë§¤ì¹­
        theory_concepts = self._pattern_cache.get('theory_concepts', [])
        if query in theory_concepts:
            for concept in theory_concepts:
                if concept in content:
                    score += 0.8

        return score

    def _context_based_search(self, query: str, k: int) -> List[Dict]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ìš©ì–´ì§‘ ê¸°ë°˜)"""
        context_results = []
        query_context = self._analyze_query_context_with_terms(query)

        for i, chunk in enumerate(self.chunks):
            content = chunk['content']
            metadata = chunk['metadata']
            score = 0.0

            # ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ (ìš©ì–´ì§‘ ê¸°ë°˜)
            if query_context['is_symptom'] and self._contains_symptom_patterns(content):
                score += 2.0
            if query_context['is_prescription'] and metadata.get('type') == 'prescription':
                score += 3.0
            if query_context['is_theory'] and metadata.get('BB'):
                score += 1.5
            if query_context['is_herb'] and self._contains_herb_patterns(content):
                score += 2.5

            # ê´€ë ¨ ìš©ì–´ ë§¤ì¹­ (ìš©ì–´ì§‘ ê¸°ë°˜)
            related_terms = query_context.get('related_terms', [])
            for term in related_terms:
                if term in content:
                    score += 0.8

            # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if query_context['category'] and query_context['category'] == metadata.get('BB'):
                score += 2.0

            # ìš©ì–´ì§‘ ê¸°ë°˜ í™•ì¥ ë§¤ì¹­
            expanded_terms = query_context.get('expanded_terms', [])
            for term in expanded_terms:
                if term in content:
                    score += 0.6

            if score > 0.5:
                context_results.append({
                    'content': content,
                    'metadata': metadata,
                    'semantic_score': score,
                    'chunk_index': i
                })

        context_results.sort(key=lambda x: x['semantic_score'], reverse=True)
        return context_results[:k]

    def _contains_symptom_patterns(self, content: str) -> bool:
        """ì¦ìƒ íŒ¨í„´ í¬í•¨ ì—¬ë¶€ (ë™ì )"""
        symptom_suffixes = self._pattern_cache.get('symptom_suffixes', [])
        return any(suffix in content for suffix in symptom_suffixes)

    def _contains_herb_patterns(self, content: str) -> bool:
        """ì•½ë¬¼ íŒ¨í„´ í¬í•¨ ì—¬ë¶€ (ë™ì )"""
        major_herbs = self._pattern_cache.get('major_herbs', [])
        return any(herb in content for herb in major_herbs)

    def expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥ (í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜)"""
        try:
            if self.terms_manager:
                # í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ í™•ì¥
                standard_expansions = self.terms_manager.expand_query(
                    query, max_expansions=10)
                query_parts = self.terms_manager.split_query_intelligently(
                    query)
                for part in query_parts:
                    if part not in standard_expansions:
                        standard_expansions.append(part)
                return standard_expansions[:12]
        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ í™•ì¥ ì‹¤íŒ¨: {e}")

        # í´ë°±: ê¸°ë³¸ í™•ì¥
        return [query]

    def _collect_comprehensive_candidates(self, query: str, k: int) -> List[Dict]:
        """í¬ê´„ì  í›„ë³´ ìˆ˜ì§‘"""
        all_candidates = []

        # ì§ì ‘ ë§¤ì¹­
        direct_results = self._semantic_search(query, k * 8)
        for r in direct_results:
            r['search_strategy'] = 'direct'
            r['relevance_boost'] = 1.0
        all_candidates.extend(direct_results)

        # í™•ì¥ ê²€ìƒ‰
        expanded_queries = self.expand_query(query)
        for i, exp_query in enumerate(expanded_queries[1:]):
            exp_results = self._semantic_search(exp_query, k * 3)
            for r in exp_results:
                r['search_strategy'] = 'expanded'
                r['expanded_query'] = exp_query
                r['relevance_boost'] = 0.9 - (i * 0.02)
            all_candidates.extend(exp_results)

        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self._enhanced_keyword_search(query, k * 6)
        for r in keyword_results:
            r['search_strategy'] = 'keyword'
            r['relevance_boost'] = 0.95
        all_candidates.extend(keyword_results)

        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context_results = self._context_based_search(query, k * 4)
        for r in context_results:
            r['search_strategy'] = 'context'
            r['relevance_boost'] = 0.85
        all_candidates.extend(context_results)

        print(f"ğŸ“Š ì´ {len(all_candidates)}ê°œ í›„ë³´ ìˆ˜ì§‘")

        # ê³ ê¸‰ ìŠ¤ì½”ì–´ë§
        scored_results = self._enhanced_scoring(query, all_candidates)
        return scored_results[:k * 20]

    def _analyze_query_context_with_terms(self, query: str) -> Dict:
        """ì¿¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (ìš©ì–´ì§‘ ê¸°ë°˜)"""
        context = {
            'is_symptom': False,
            'is_prescription': False,
            'is_theory': False,
            'is_herb': False,
            'category': None,
            'related_terms': [],
            'expanded_terms': []
        }

        try:
            # í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ë¶„ì„
            if self.terms_manager:
                term_info = self.terms_manager.get_term_info(query)
                if term_info:
                    category = term_info.get('ë¶„ë¥˜', '')
                    context['category'] = category

                    # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ íƒ€ì… ì„¤ì •
                    if category == 'ë³‘ì¦':
                        context['is_symptom'] = True
                    elif category == 'ì²˜ë°©':
                        context['is_prescription'] = True
                    elif category in ['ìƒë¦¬', 'ë³‘ë¦¬']:
                        context['is_theory'] = True
                    elif category == 'ì•½ë¬¼':
                        context['is_herb'] = True

                # ê´€ë ¨ ìš©ì–´ ë° í™•ì¥ ìš©ì–´
                context['related_terms'] = self.terms_manager.get_related_terms(
                    query)
                context['expanded_terms'] = self.terms_manager.expand_query(
                    query, max_expansions=5)

        except Exception as e:
            print(f"âš ï¸ ìš©ì–´ì§‘ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")

        # í´ë°±: íŒ¨í„´ ê¸°ë°˜ ë¶„ì„
        if not any([context['is_symptom'], context['is_prescription'], context['is_theory'], context['is_herb']]):
            context.update(self._analyze_query_context_fallback(query))

        return context

    def _analyze_query_context_fallback(self, query: str) -> Dict:
        """í´ë°± ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            'is_symptom': False,
            'is_prescription': False,
            'is_theory': False,
            'is_herb': False,
            'related_terms': []
        }

        # íŒ¨í„´ ê¸°ë°˜ ê°ì§€
        prescription_suffixes = self._pattern_cache.get(
            'prescription_suffixes', [])
        if any(query.endswith(suffix) for suffix in prescription_suffixes):
            context['is_prescription'] = True

        symptom_suffixes = self._pattern_cache.get('symptom_suffixes', [])
        if any(query.endswith(suffix) for suffix in symptom_suffixes):
            context['is_symptom'] = True

        theory_concepts = self._pattern_cache.get('theory_concepts', [])
        if query in theory_concepts:
            context['is_theory'] = True

        major_herbs = self._pattern_cache.get('major_herbs', [])
        if query in major_herbs:
            context['is_herb'] = True

        # ê¸°ë³¸ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ
        context['related_terms'] = self._extract_basic_related_terms_improved(
            query)

        return context

    def _extract_basic_related_terms_improved(self, query: str) -> List[str]:
        """ê°œì„ ëœ ê¸°ë³¸ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ (ë™ì  íŒ¨í„´ ê¸°ë°˜)"""
        basic_relations = []

        # ë™ì  íŒ¨í„´ ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨
        if 'è™›' in query:
            è£œ_terms = ['è£œ', 'ç›Š', 'é¤Š', 'èª¿', 'æº«']
            # ìš©ì–´ì§‘ì—ì„œ è£œ ê´€ë ¨ ì²˜ë°©ë“¤ ì¶”ê°€
            if self.terms_manager:
                try:
                    è¡¥_prescriptions = []
                    for term in self.terms_manager.search_index.keys():
                        if 'è£œ' in term and any(suffix in term for suffix in ['æ¹¯', 'æ•£', 'ä¸¸']):
                            è¡¥_prescriptions.append(term)
                    basic_relations.extend(è¡¥_prescriptions[:3])
                except:
                    pass
            basic_relations.extend(è£œ_terms)

        if 'è¡€' in query:
            è¡€_terms = ['æ°£', 'é™°', 'é™½', 'è£œè¡€', 'é¤Šè¡€']
            # ìš©ì–´ì§‘ì—ì„œ è¡€ ê´€ë ¨ ìš©ì–´ë“¤ ì¶”ê°€
            if self.terms_manager:
                try:
                    blood_terms = []
                    for term in self.terms_manager.search_index.keys():
                        if 'è¡€' in term and term != query:
                            blood_terms.append(term)
                    basic_relations.extend(blood_terms[:3])
                except:
                    pass
            basic_relations.extend(è¡€_terms)

        if 'æ°£' in query:
            æ°£_terms = ['è¡€', 'è£œæ°£', 'ç›Šæ°£', 'ç†æ°£']
            # ìš©ì–´ì§‘ì—ì„œ æ°£ ê´€ë ¨ ìš©ì–´ë“¤ ì¶”ê°€
            if self.terms_manager:
                try:
                    qi_terms = []
                    for term in self.terms_manager.search_index.keys():
                        if 'æ°£' in term and term != query:
                            qi_terms.append(term)
                    basic_relations.extend(qi_terms[:3])
                except:
                    pass
            basic_relations.extend(æ°£_terms)

        if 'æ¹¯' in query:
            ì²˜ë°©_terms = ['è™•æ–¹', 'æ–¹åŠ‘', 'æ²»ç™‚']
            # ê°™ì€ ê³„ì—´ ì²˜ë°©ë“¤ ì¶”ê°€
            if self.terms_manager:
                try:
                    similar_prescriptions = []
                    for term in self.terms_manager.search_index.keys():
                        if 'æ¹¯' in term and term != query:
                            similar_prescriptions.append(term)
                    basic_relations.extend(similar_prescriptions[:3])
                except:
                    pass
            basic_relations.extend(ì²˜ë°©_terms)

        return basic_relations

    def _cluster_by_content_type(self, query: str, candidates: List[Dict]) -> Dict[str, List[Dict]]:
        """ë‚´ìš© íƒ€ì…ë³„ í´ëŸ¬ìŠ¤í„°ë§"""
        clusters = {
            'direct_match': [],      # ì§ì ‘ ë§¤ì¹­
            'prescription': [],      # ì²˜ë°© ê´€ë ¨
            'symptom_theory': [],    # ì¦ìƒ/ì´ë¡ 
            'related_concept': [],   # ê´€ë ¨ ê°œë…
            'treatment_method': [],  # ì¹˜ë£Œë²•
            'differential': []       # ê°ë³„ì§„ë‹¨
        }

        for candidate in candidates:
            content = candidate['content']
            metadata = candidate['metadata']

            # í´ëŸ¬ìŠ¤í„° ë¶„ë¥˜ (ê°œì„ ëœ ë¡œì§)
            if query in content and candidate.get('semantic_score', 0) > 4.0:
                clusters['direct_match'].append(candidate)
            elif metadata.get('type') == 'prescription':
                clusters['prescription'].append(candidate)
            elif self._is_symptom_theory_content(content):
                clusters['symptom_theory'].append(candidate)
            elif self._is_treatment_method_content(content):
                clusters['treatment_method'].append(candidate)
            elif self._is_related_concept_improved(query, content):
                clusters['related_concept'].append(candidate)
            else:
                clusters['differential'].append(candidate)

        # ê° í´ëŸ¬ìŠ¤í„° ë‚´ ì •ë ¬
        for cluster_name, cluster_items in clusters.items():
            clusters[cluster_name] = sorted(cluster_items,
                                            key=lambda x: x.get(
                                                'final_score', x.get('semantic_score', 0)),
                                            reverse=True)

        print(f"ğŸ“‹ í´ëŸ¬ìŠ¤í„° ë¶„í¬: ì§ì ‘ë§¤ì¹­={len(clusters['direct_match'])}, "
              f"ì²˜ë°©={len(clusters['prescription'])}, ì¦ìƒì´ë¡ ={len(clusters['symptom_theory'])}, "
              f"ì¹˜ë£Œë²•={len(clusters['treatment_method'])}, ê´€ë ¨ê°œë…={len(clusters['related_concept'])}, "
              f"ê¸°íƒ€={len(clusters['differential'])}")

        return clusters

    def _is_symptom_theory_content(self, content: str) -> bool:
        """ì¦ìƒ/ì´ë¡  ë‚´ìš© íŒë‹¨ (ë™ì  íŒ¨í„´)"""
        symptom_keywords = ['è­‰', 'ç—…', 'ç—‡', 'ç—›']
        symptom_suffixes = self._pattern_cache.get('symptom_suffixes', [])
        all_symptom_indicators = symptom_keywords + symptom_suffixes
        return any(keyword in content for keyword in all_symptom_indicators)

    def _is_treatment_method_content(self, content: str) -> bool:
        """ì¹˜ë£Œë²• ë‚´ìš© íŒë‹¨"""
        treatment_keywords = ['æ²»', 'ç™‚', 'ä¸»', 'ç”¨', 'æ³•', 'æ–¹æ³•']
        return any(keyword in content for keyword in treatment_keywords)

    def _is_related_concept_improved(self, query: str, content: str) -> bool:
        """ê´€ë ¨ ê°œë… íŒë‹¨ (ìš©ì–´ì§‘ ê¸°ë°˜)"""
        try:
            if self.terms_manager:
                related_terms = self.terms_manager.get_related_terms(query)
                return any(term in content for term in related_terms)
        except Exception:
            pass

        # í´ë°±: ë™ì  íŒ¨í„´ ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨
        if any(char in query for char in ['è™›', 'æ°£', 'è¡€', 'é™°', 'é™½']):
            related_patterns = self._pattern_cache.get('theory_concepts', [])
            related_patterns.extend(['è™›', 'æ°£', 'è¡€', 'é™°', 'é™½', 'è£œ', 'ç›Š', 'é¤Š'])
            return any(pattern in content for pattern in related_patterns)

        return False

    def _enhanced_scoring(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ê°•í™”ëœ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ"""
        for candidate in candidates:
            final_score = 0.0

            # ê¸°ë³¸ ì‹œë§¨í‹± ì ìˆ˜
            semantic_score = candidate.get('semantic_score', 0.0)
            final_score += semantic_score * 0.3

            # ê²€ìƒ‰ ì „ëµë³„ ê°€ì¤‘ì¹˜
            strategy_weight = {
                'direct': 1.0,
                'expanded': 0.8,
                'keyword': 0.9,
                'context': 0.7
            }
            strategy = candidate.get('search_strategy', 'direct')
            final_score *= strategy_weight.get(strategy, 0.5)

            # ê´€ë ¨ì„± ë¶€ìŠ¤íŠ¸
            relevance_boost = candidate.get('relevance_boost', 1.0)
            final_score *= relevance_boost

            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            keyword_bonus = self._calculate_keyword_bonus(query, candidate)
            final_score += keyword_bonus * 0.4

            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³´ë„ˆìŠ¤
            context_bonus = self._calculate_context_bonus(candidate)
            final_score += context_bonus * 0.2

            # íƒ€ì… ë³´ë„ˆìŠ¤
            type_bonus = self._calculate_type_bonus(query, candidate)
            final_score += type_bonus * 0.1

            candidate['final_score'] = final_score

        return sorted(candidates, key=lambda x: x['final_score'], reverse=True)

    def _calculate_keyword_bonus(self, query: str, candidate: Dict) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        content = candidate['content'].lower()
        query_lower = query.lower()
        bonus = 0.0

        # ì™„ì „ ë§¤ì¹­
        if query_lower in content:
            bonus += 2.0

        # ì²˜ë°©ëª… ë§¤ì¹­
        prescription_name = candidate['metadata'].get(
            'prescription_name', '').lower()
        if prescription_name and (query_lower in prescription_name or prescription_name in query_lower):
            bonus += 3.0

        # ê¸€ìë³„ ë§¤ì¹­
        matched_chars = sum(1 for char in query_lower if char in content)
        char_ratio = matched_chars / len(query_lower) if query_lower else 0
        bonus += char_ratio * 0.5

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë§¤ì¹­
        keywords = candidate['metadata'].get('keywords', [])
        for keyword in keywords:
            if keyword.lower() in query_lower or query_lower in keyword.lower():
                bonus += 0.3

        # ìš©ì–´ì§‘ ê¸°ë°˜ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if self.terms_manager:
            try:
                expanded_terms = self.terms_manager.expand_query(
                    query, max_expansions=5)
                for exp_term in expanded_terms:
                    if exp_term.lower() in content:
                        bonus += 0.4
            except:
                pass

        return min(bonus, 3.0)

    def _calculate_context_bonus(self, candidate: Dict) -> float:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³´ë„ˆìŠ¤"""
        bonus = 0.0
        metadata = candidate['metadata']

        # ë‚´ìš© ê¸¸ì´ ì ì •ì„±
        content_len = len(candidate['content'])
        if 200 <= content_len <= 1000:
            bonus += 1.0
        elif 100 <= content_len <= 1500:
            bonus += 0.5

        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        if metadata.get('BB'):
            bonus += 0.3
        if metadata.get('CC'):
            bonus += 0.3
        if metadata.get('prescription_name'):
            bonus += 0.4

        return bonus

    def _calculate_type_bonus(self, query: str, candidate: Dict) -> float:
        """ë¬¸ì„œ íƒ€ì… ë³´ë„ˆìŠ¤"""
        content_type = candidate['metadata'].get('type', 'general')

        # ì²˜ë°© ê´€ë ¨ ì¿¼ë¦¬ ê°ì§€ (ë™ì  íŒ¨í„´ ì‚¬ìš©)
        prescription_suffixes = self._pattern_cache.get(
            'prescription_suffixes', [])
        is_prescription_query = any(query.endswith(suffix)
                                    for suffix in prescription_suffixes)

        if is_prescription_query and content_type == 'prescription':
            return 1.0
        elif content_type == 'prescription':
            return 0.5
        else:
            return 0.0

    def _select_representatives(self, clusters: Dict[str, List[Dict]], k: int) -> List[Dict]:
        """ëŒ€í‘œ ì„ ì • (ìƒìš© AI ëª¨ë¸ìš©)"""

        # ìƒìš© AI ëª¨ë¸ìš© í• ë‹¹ ì „ëµ
        if k < 50:
            allocation_strategy = {
                'direct_match': 0.35,     # 35% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.05,  # 5% - ê´€ë ¨ ê°œë…
                'differential': 0.0       # 0% - ê¸°íƒ€
            }
        elif k <= 70:
            allocation_strategy = {
                'direct_match': 0.30,     # 30% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.07,  # 7% - ê´€ë ¨ ê°œë…
                'differential': 0.03      # 3% - ê°ë³„ì§„ë‹¨
            }
        else:
            allocation_strategy = {
                'direct_match': 0.25,     # 25% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.10,  # 10% - ê´€ë ¨ ê°œë…
                'differential': 0.05      # 5% - ê°ë³„ì§„ë‹¨
            }

        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í›„ë³´ ìˆ˜ í™•ì¸
        total_available = sum(len(cluster_items)
                              for cluster_items in clusters.values())
        actual_k = min(k, total_available)

        if actual_k < k:
            print(f"âš ï¸ ìš”ì²­ëœ Kê°’({k})ë³´ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•œ í›„ë³´ê°€ ì ìŠµë‹ˆë‹¤({total_available}ê°œ)")
            print(f"ğŸ“Š ì‹¤ì œ ì„ ì • ê°€ëŠ¥í•œ ìˆ˜: {actual_k}ê°œ")

        selected = []
        used_content_hashes = set()
        cluster_stats = {}

        # 1ë‹¨ê³„: ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ í• ë‹¹ëœ ë¹„ìœ¨ë§Œí¼ ì„ ì •
        for cluster_name, ratio in allocation_strategy.items():
            cluster_items = clusters.get(cluster_name, [])

            if not cluster_items:
                cluster_stats[cluster_name] = {
                    'target': 0, 'selected': 0, 'available': 0}
                continue

            target_count = max(1, int(actual_k * ratio)) if ratio > 0 else 0

            cluster_selected = 0
            for item in cluster_items:
                if cluster_selected >= target_count or len(selected) >= actual_k:
                    break

                content_hash = hash(item['content'])
                if content_hash in used_content_hashes:
                    continue

                selected.append(item)
                used_content_hashes.add(content_hash)
                cluster_selected += 1

            cluster_stats[cluster_name] = {
                'target': target_count,
                'selected': cluster_selected,
                'available': len(cluster_items)
            }

            if len(selected) >= actual_k:
                break

        # 2ë‹¨ê³„: ë¶€ì¡±í•œ ê²½ìš° ìµœê³  ì ìˆ˜ í•­ëª©ë“¤ë¡œ ì±„ìš°ê¸°
        if len(selected) < actual_k:
            print(f"ğŸ”„ {len(selected)}/{actual_k}ê°œ ì„ ì •ë¨. ì¶”ê°€ í•­ëª©ìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")

            all_remaining = []
            for cluster_items in clusters.values():
                all_remaining.extend(cluster_items)

            all_remaining.sort(key=lambda x: x.get(
                'final_score', x.get('semantic_score', 0)), reverse=True)

            for item in all_remaining:
                if len(selected) >= actual_k:
                    break

                content_hash = hash(item['content'])
                if content_hash not in used_content_hashes:
                    selected.append(item)
                    used_content_hashes.add(content_hash)

        # 3ë‹¨ê³„: ëŒ€ìš©ëŸ‰ì—ì„œ ë‹¤ì–‘ì„± ë³´ì¥ (Kâ‰¥50)
        if actual_k >= 50:
            diversity_enhanced = self._ensure_diversity_for_large_k(
                selected, actual_k)
            selected = diversity_enhanced

        # 4ë‹¨ê³„: ìµœì¢… ì •ë ¬ ë° score í•„ë“œ ì„¤ì •
        for item in selected:
            if 'score' not in item:
                item['score'] = item.get(
                    'final_score', item.get('semantic_score', 0))

        final_selected = sorted(
            selected, key=lambda x: x['score'], reverse=True)[:actual_k]

        # ìƒì„¸ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ì„ ì • ê²°ê³¼:")
        for cluster_name, stats in cluster_stats.items():
            print(f"   {cluster_name}: {stats['selected']}/{stats['target']}ê°œ ì„ ì • "
                  f"(ì‚¬ìš©ê°€ëŠ¥: {stats['available']}ê°œ)")

        print(f"\nğŸ¯ ìµœì¢… ì„ ì •: {len(final_selected)}ê°œ (ìš”ì²­: {k}ê°œ, ì‹¤ì œ: {actual_k}ê°œ)")

        return final_selected

    def _ensure_diversity_for_large_k(self, selected: List[Dict], k: int) -> List[Dict]:
        """ëŒ€ìš©ëŸ‰ Kê°’(50+)ì—ì„œ ë‹¤ì–‘ì„± ë³´ì¥"""

        if k < 50:
            return selected

        # ì¶œì²˜ íŒŒì¼ë³„ ë¶„í¬ ìµœì í™”
        max_per_source = max(5, k // 8)  # ìµœëŒ€ 12.5% ë˜ëŠ” ìµœì†Œ 5ê°œ
        # ëŒ€ë¶„ë¥˜(BB)ë³„ ë¶„í¬ ì œí•œ
        max_per_bb = max(8, k // 6)  # ìµœëŒ€ 16.7% ë˜ëŠ” ìµœì†Œ 8ê°œ

        adjusted_selected = []
        source_counts = {}
        bb_counts = {}

        # 1ì°¨: ì¶œì²˜ì™€ ëŒ€ë¶„ë¥˜ ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì„ ì •
        for item in sorted(selected, key=lambda x: x.get('score', 0), reverse=True):
            source = item['metadata'].get('source_file', 'unknown')
            bb = item['metadata'].get('BB', 'unknown')

            source_count = source_counts.get(source, 0)
            bb_count = bb_counts.get(bb, 0)

            if source_count < max_per_source and bb_count < max_per_bb:
                adjusted_selected.append(item)
                source_counts[source] = source_count + 1
                bb_counts[bb] = bb_count + 1

            if len(adjusted_selected) >= k:
                break

        # 2ì°¨: ë¶€ì¡±í•œ ê²½ìš° ì œí•œì„ ì™„í™”í•˜ì—¬ ì±„ìš°ê¸°
        if len(adjusted_selected) < k:
            remaining = [
                item for item in selected if item not in adjusted_selected]
            needed = k - len(adjusted_selected)

            remaining.sort(key=lambda x: x.get('score', 0), reverse=True)
            adjusted_selected.extend(remaining[:needed])

        # í†µê³„ ì •ë³´ ì¶œë ¥
        final_source_dist = {}
        final_bb_dist = {}
        final_type_dist = {}

        for item in adjusted_selected:
            source = item['metadata'].get('source_file', 'unknown')
            bb = item['metadata'].get('BB', 'unknown')
            content_type = item['metadata'].get('type', 'general')

            final_source_dist[source] = final_source_dist.get(source, 0) + 1
            final_bb_dist[bb] = final_bb_dist.get(bb, 0) + 1
            final_type_dist[content_type] = final_type_dist.get(
                content_type, 0) + 1

        print(
            f"ğŸ“š ì¶œì²˜ ë‹¤ì–‘ì„±: {len(final_source_dist)}ê°œ íŒŒì¼ (ìµœëŒ€ {max_per_source}ê°œ/íŒŒì¼)")
        print(f"ğŸ“– ëŒ€ë¶„ë¥˜ ë‹¤ì–‘ì„±: {len(final_bb_dist)}ê°œ ì˜ì—­ (ìµœëŒ€ {max_per_bb}ê°œ/ì˜ì—­)")
        print(f"ğŸ·ï¸ ë‚´ìš© íƒ€ì…: {final_type_dist}")

        return adjusted_selected

    def get_pattern_cache_info(self) -> Dict:
        """íŒ¨í„´ ìºì‹œ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        return {
            'prescription_suffixes_count': len(self._pattern_cache.get('prescription_suffixes', [])),
            'symptom_suffixes_count': len(self._pattern_cache.get('symptom_suffixes', [])),
            'major_herbs_count': len(self._pattern_cache.get('major_herbs', [])),
            'theory_concepts_count': len(self._pattern_cache.get('theory_concepts', [])),
            'terms_manager_connected': self.terms_manager is not None
        }

    def clear_pattern_cache(self):
        """íŒ¨í„´ ìºì‹œ ì´ˆê¸°í™”"""
        self._pattern_cache.clear()
        self._relation_cache.clear()
        if self.terms_manager:
            self._build_dynamic_patterns()

    def rebuild_patterns(self):
        """íŒ¨í„´ ì¬êµ¬ì¶•"""
        print("ğŸ”„ ë™ì  íŒ¨í„´ ì¬êµ¬ì¶• ì¤‘...")
        self.clear_pattern_cache()
        print("âœ… ë™ì  íŒ¨í„´ ì¬êµ¬ì¶• ì™„ë£Œ")
