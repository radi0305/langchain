#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ì˜ë³´ê° ê²€ìƒ‰ ì—”ì§„ ëª¨ë“ˆ - search_engine.py
ì„ë² ë”©, ì¸ë±ì‹±, ê²€ìƒ‰ ë¡œì§ì„ ë‹´ë‹¹
"""

import numpy as np
import faiss
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings("ignore")


class SearchEngine:
    def __init__(self, embedding_model_name: str = 'paraphrase-multilingual-mpnet-base-v2'):
        """ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedding_model = SentenceTransformer(embedding_model_name)
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        self.chunks = []
        self.embeddings = None
        self.faiss_index = None
        self.terms_manager = None

    def set_terms_manager(self, terms_manager):
        """í‘œì¤€ìš©ì–´ì§‘ ê´€ë¦¬ì ì„¤ì •"""
        self.terms_manager = terms_manager

    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:
        """ì„ë² ë”© ìƒì„±"""
        print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")

        texts = []
        for chunk in chunks:
            title_parts = []
            if chunk['metadata'].get('BB'):
                title_parts.append(chunk['metadata']['BB'])
            if chunk['metadata'].get('CC'):
                title_parts.append(chunk['metadata']['CC'])

            title = ' '.join(title_parts)
            combined_text = f"{title} {chunk['content']}" if title else chunk['content']
            texts.append(combined_text)

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        batch_size = 32
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(
                batch, show_progress_bar=True, batch_size=batch_size)
            embeddings.append(batch_embeddings)

        embeddings = np.vstack(embeddings)
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
        return embeddings

    def build_faiss_index(self, embeddings: np.ndarray):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ” FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)

        # ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        faiss.normalize_L2(embeddings)
        index.add(embeddings.astype('float32'))

        print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„°")
        return index

    def setup(self, chunks: List[Dict], embeddings: np.ndarray = None):
        """ê²€ìƒ‰ ì—”ì§„ ì„¤ì •"""
        self.chunks = chunks

        if embeddings is None:
            self.embeddings = self.create_embeddings(chunks)
        else:
            self.embeddings = embeddings

        self.faiss_index = self.build_faiss_index(self.embeddings)

    def search(self, query: str, k: int = 75) -> List[Dict]:
        """ë©”ì¸ ê²€ìƒ‰ í•¨ìˆ˜"""
        if self.faiss_index is None:
            raise ValueError("ê²€ìƒ‰ ì—”ì§„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print(f"ğŸ” '{query}' ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ ì¤‘...")

        # 1ë‹¨ê³„: ëŒ€ëŸ‰ í›„ë³´ ìˆ˜ì§‘
        all_candidates = self._collect_comprehensive_candidates(query, k)

        # 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        clustered_candidates = self._cluster_by_content_type(
            query, all_candidates)

        # 3ë‹¨ê³„: ëŒ€í‘œ ì„ ì •
        final_results = self._select_representatives(clustered_candidates, k)

        print(f"âœ… ìµœì¢… {len(final_results)}ê°œ ëŒ€í‘œ ê²°ê³¼ ì„ ì •")
        return final_results

    def _semantic_search(self, query: str, k: int) -> List[Dict]:
        """ê¸°ë³¸ ì‹œë§¨í‹± ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.faiss_index.search(
            query_embedding.astype('float32'), k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append({
                    'content': chunk['content'],
                    'metadata': chunk['metadata'],
                    'semantic_score': float(score),
                    'chunk_index': idx
                })

        return results

    def _enhanced_keyword_search(self, query: str, k: int) -> List[Dict]:
        """ê°•í™”ëœ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        keyword_results = []

        for i, chunk in enumerate(self.chunks):
            content = chunk['content']
            score = 0.0

            # ì™„ì „ ë§¤ì¹­
            if query in content:
                score += 5.0

            # ë¶€ë¶„ ë§¤ì¹­
            for char in query:
                if char in content:
                    score += 0.2

            # ì²˜ë°©ëª… ë§¤ì¹­
            if chunk['metadata'].get('prescription_name'):
                prescription_name = chunk['metadata']['prescription_name']
                if query in prescription_name or prescription_name in query:
                    score += 8.0

            # í‚¤ì›Œë“œ ë§¤ì¹­
            keywords = chunk['metadata'].get('keywords', [])
            for keyword in keywords:
                if query in keyword or keyword in query:
                    score += 1.0

            # ìœ„ì¹˜ ê°€ì¤‘ì¹˜
            if query in content:
                position = content.find(query)
                position_weight = max(0, 1.0 - (position / len(content)))
                score += position_weight * 2.0

            if score > 1.0:
                keyword_results.append({
                    'content': content,
                    'metadata': chunk['metadata'],
                    'semantic_score': score,
                    'chunk_index': i
                })

        keyword_results.sort(key=lambda x: x['semantic_score'], reverse=True)
        return keyword_results[:k]

    def _context_based_search(self, query: str, k: int) -> List[Dict]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰"""
        context_results = []
        query_context = self._analyze_query_context(query)

        for i, chunk in enumerate(self.chunks):
            content = chunk['content']
            metadata = chunk['metadata']
            score = 0.0

            # ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­
            if query_context['is_symptom'] and 'è­‰' in content:
                score += 2.0
            if query_context['is_prescription'] and metadata.get('type') == 'prescription':
                score += 3.0
            if query_context['is_theory'] and metadata.get('BB'):
                score += 1.5

            # ê´€ë ¨ ìš©ì–´ ë§¤ì¹­
            related_terms = query_context.get('related_terms', [])
            for term in related_terms:
                if term in content:
                    score += 0.8

            # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if query_context['category'] and query_context['category'] == metadata.get('BB'):
                score += 2.0

            if score > 0.5:
                context_results.append({
                    'content': content,
                    'metadata': metadata,
                    'semantic_score': score,
                    'chunk_index': i
                })

        context_results.sort(key=lambda x: x['semantic_score'], reverse=True)
        return context_results[:k]

    def expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥"""
        try:
            if self.terms_manager:
                # í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ í™•ì¥
                standard_expansions = self.terms_manager.expand_query(
                    query, max_expansions=10)
                query_parts = self.terms_manager.split_query_intelligently(
                    query)
                for part in query_parts:
                    if part not in standard_expansions:
                        standard_expansions.append(part)
                return standard_expansions[:12]
        except Exception as e:
            print(f"âš ï¸ í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ í™•ì¥ ì‹¤íŒ¨: {e}")

        # í´ë°±: ê¸°ë³¸ í™•ì¥
        return [query]

    def _collect_comprehensive_candidates(self, query: str, k: int) -> List[Dict]:
        """í¬ê´„ì  í›„ë³´ ìˆ˜ì§‘"""
        all_candidates = []

        # ì§ì ‘ ë§¤ì¹­
        direct_results = self._semantic_search(query, k * 8)
        for r in direct_results:
            r['search_strategy'] = 'direct'
            r['relevance_boost'] = 1.0
        all_candidates.extend(direct_results)

        # í™•ì¥ ê²€ìƒ‰
        expanded_queries = self.expand_query(query)
        for i, exp_query in enumerate(expanded_queries[1:]):
            exp_results = self._semantic_search(exp_query, k * 3)
            for r in exp_results:
                r['search_strategy'] = 'expanded'
                r['expanded_query'] = exp_query
                r['relevance_boost'] = 0.9 - (i * 0.02)
            all_candidates.extend(exp_results)

        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self._enhanced_keyword_search(query, k * 6)
        for r in keyword_results:
            r['search_strategy'] = 'keyword'
            r['relevance_boost'] = 0.95
        all_candidates.extend(keyword_results)

        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context_results = self._context_based_search(query, k * 4)
        for r in context_results:
            r['search_strategy'] = 'context'
            r['relevance_boost'] = 0.85
        all_candidates.extend(context_results)

        print(f"ğŸ“Š ì´ {len(all_candidates)}ê°œ í›„ë³´ ìˆ˜ì§‘")

        # ê³ ê¸‰ ìŠ¤ì½”ì–´ë§
        scored_results = self._enhanced_scoring(query, all_candidates)
        return scored_results[:k * 20]

    def _analyze_query_context(self, query: str) -> Dict:
        """ì¿¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            'is_symptom': False,
            'is_prescription': False,
            'is_theory': False,
            'category': None,
            'related_terms': []
        }

        # ì¦ìƒ/ì²˜ë°©/ì´ë¡  ê°ì§€
        symptom_indicators = ['è™›', 'è­‰', 'ç—…', 'ç—‡', 'ç—›', 'ç†±', 'å¯’', 'æ¿•', 'ç‡¥']
        if any(indicator in query for indicator in symptom_indicators):
            context['is_symptom'] = True

        prescription_indicators = ['æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'è™•æ–¹']
        if any(indicator in query for indicator in prescription_indicators):
            context['is_prescription'] = True

        theory_indicators = ['é™°é™½', 'äº”è¡Œ', 'æ°£è¡€', 'ç²¾æ°£ç¥']
        if any(indicator in query for indicator in theory_indicators):
            context['is_theory'] = True

        # í‘œì¤€ìš©ì–´ì§‘ ê¸°ë°˜ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ
        try:
            if self.terms_manager:
                context['related_terms'] = self.terms_manager.get_related_terms(
                    query)
        except Exception:
            context['related_terms'] = self._extract_basic_related_terms(query)

        return context

    def _extract_basic_related_terms(self, query: str) -> List[str]:
        """ê¸°ë³¸ì ì¸ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ"""
        basic_relations = []

        if 'è™›' in query:
            basic_relations.extend(['è£œ', 'ç›Š', 'é¤Š', 'èª¿', 'æº«'])
        if 'è¡€' in query:
            basic_relations.extend(['æ°£', 'é™°', 'é™½', 'è£œè¡€', 'é¤Šè¡€'])
        if 'æ°£' in query:
            basic_relations.extend(['è¡€', 'è£œæ°£', 'ç›Šæ°£', 'ç†æ°£'])
        if 'æ¹¯' in query:
            basic_relations.extend(['è™•æ–¹', 'æ–¹åŠ‘', 'æ²»ç™‚'])

        return basic_relations

    def _cluster_by_content_type(self, query: str, candidates: List[Dict]) -> Dict[str, List[Dict]]:
        """ë‚´ìš© íƒ€ì…ë³„ í´ëŸ¬ìŠ¤í„°ë§"""
        clusters = {
            'direct_match': [],      # ì§ì ‘ ë§¤ì¹­
            'prescription': [],      # ì²˜ë°© ê´€ë ¨
            'symptom_theory': [],    # ì¦ìƒ/ì´ë¡ 
            'related_concept': [],   # ê´€ë ¨ ê°œë…
            'treatment_method': [],  # ì¹˜ë£Œë²•
            'differential': []       # ê°ë³„ì§„ë‹¨
        }

        for candidate in candidates:
            content = candidate['content']
            metadata = candidate['metadata']

            # í´ëŸ¬ìŠ¤í„° ë¶„ë¥˜
            if query in content and candidate.get('semantic_score', 0) > 4.0:
                clusters['direct_match'].append(candidate)
            elif metadata.get('type') == 'prescription':
                clusters['prescription'].append(candidate)
            elif any(keyword in content for keyword in ['è­‰', 'ç—…', 'ç—‡', 'ç—›']):
                clusters['symptom_theory'].append(candidate)
            elif any(keyword in content for keyword in ['æ²»', 'ç™‚', 'ä¸»', 'ç”¨']):
                clusters['treatment_method'].append(candidate)
            elif self._is_related_concept(query, content):
                clusters['related_concept'].append(candidate)
            else:
                clusters['differential'].append(candidate)

        # ê° í´ëŸ¬ìŠ¤í„° ë‚´ ì •ë ¬
        for cluster_name, cluster_items in clusters.items():
            clusters[cluster_name] = sorted(cluster_items,
                                            key=lambda x: x.get(
                                                'final_score', x.get('semantic_score', 0)),
                                            reverse=True)

        print(f"ğŸ“‹ í´ëŸ¬ìŠ¤í„° ë¶„í¬: ì§ì ‘ë§¤ì¹­={len(clusters['direct_match'])}, "
              f"ì²˜ë°©={len(clusters['prescription'])}, ì¦ìƒì´ë¡ ={len(clusters['symptom_theory'])}, "
              f"ì¹˜ë£Œë²•={len(clusters['treatment_method'])}, ê´€ë ¨ê°œë…={len(clusters['related_concept'])}, "
              f"ê¸°íƒ€={len(clusters['differential'])}")

        return clusters

    def _is_related_concept(self, query: str, content: str) -> bool:
        """ê´€ë ¨ ê°œë… íŒë‹¨"""
        try:
            if self.terms_manager:
                related_terms = self.terms_manager.get_related_terms(query)
                return any(term in content for term in related_terms)
        except Exception:
            pass

        # ê¸°ë³¸ì ì¸ ê´€ë ¨ì„± íŒë‹¨
        if any(char in query for char in ['è™›', 'æ°£', 'è¡€', 'é™°', 'é™½']):
            related_patterns = ['è™›', 'æ°£', 'è¡€', 'é™°', 'é™½', 'è£œ', 'ç›Š', 'é¤Š']
            return any(pattern in content for pattern in related_patterns)

        return False

    def _enhanced_scoring(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """ê°•í™”ëœ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ"""
        for candidate in candidates:
            final_score = 0.0

            # ê¸°ë³¸ ì‹œë§¨í‹± ì ìˆ˜
            semantic_score = candidate.get('semantic_score', 0.0)
            final_score += semantic_score * 0.3

            # ê²€ìƒ‰ ì „ëµë³„ ê°€ì¤‘ì¹˜
            strategy_weight = {
                'direct': 1.0,
                'expanded': 0.8,
                'keyword': 0.9,
                'context': 0.7
            }
            strategy = candidate.get('search_strategy', 'direct')
            final_score *= strategy_weight.get(strategy, 0.5)

            # ê´€ë ¨ì„± ë¶€ìŠ¤íŠ¸
            relevance_boost = candidate.get('relevance_boost', 1.0)
            final_score *= relevance_boost

            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            keyword_bonus = self._calculate_keyword_bonus(query, candidate)
            final_score += keyword_bonus * 0.4

            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³´ë„ˆìŠ¤
            context_bonus = self._calculate_context_bonus(candidate)
            final_score += context_bonus * 0.2

            # íƒ€ì… ë³´ë„ˆìŠ¤
            type_bonus = self._calculate_type_bonus(query, candidate)
            final_score += type_bonus * 0.1

            candidate['final_score'] = final_score

        return sorted(candidates, key=lambda x: x['final_score'], reverse=True)

    def _calculate_keyword_bonus(self, query: str, candidate: Dict) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        content = candidate['content'].lower()
        query_lower = query.lower()
        bonus = 0.0

        # ì™„ì „ ë§¤ì¹­
        if query_lower in content:
            bonus += 2.0

        # ì²˜ë°©ëª… ë§¤ì¹­
        prescription_name = candidate['metadata'].get(
            'prescription_name', '').lower()
        if prescription_name and (query_lower in prescription_name or prescription_name in query_lower):
            bonus += 3.0

        # ê¸€ìë³„ ë§¤ì¹­
        matched_chars = sum(1 for char in query_lower if char in content)
        char_ratio = matched_chars / len(query_lower) if query_lower else 0
        bonus += char_ratio * 0.5

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë§¤ì¹­
        keywords = candidate['metadata'].get('keywords', [])
        for keyword in keywords:
            if keyword.lower() in query_lower or query_lower in keyword.lower():
                bonus += 0.3

        return min(bonus, 3.0)

    def _calculate_context_bonus(self, candidate: Dict) -> float:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ë³´ë„ˆìŠ¤"""
        bonus = 0.0
        metadata = candidate['metadata']

        # ë‚´ìš© ê¸¸ì´ ì ì •ì„±
        content_len = len(candidate['content'])
        if 200 <= content_len <= 1000:
            bonus += 1.0
        elif 100 <= content_len <= 1500:
            bonus += 0.5

        # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        if metadata.get('BB'):
            bonus += 0.3
        if metadata.get('CC'):
            bonus += 0.3
        if metadata.get('prescription_name'):
            bonus += 0.4

        return bonus

    def _calculate_type_bonus(self, query: str, candidate: Dict) -> float:
        """ë¬¸ì„œ íƒ€ì… ë³´ë„ˆìŠ¤"""
        content_type = candidate['metadata'].get('type', 'general')

        # ì²˜ë°© ê´€ë ¨ ì¿¼ë¦¬ ê°ì§€
        prescription_keywords = ['æ¹¯', 'æ•£', 'ä¸¸', 'è†', 'è™•æ–¹', 'ì¹˜ë£Œ']
        is_prescription_query = any(
            kw in query for kw in prescription_keywords)

        if is_prescription_query and content_type == 'prescription':
            return 1.0
        elif content_type == 'prescription':
            return 0.5
        else:
            return 0.0

    def _select_representatives(self, clusters: Dict[str, List[Dict]], k: int) -> List[Dict]:
        """ëŒ€í‘œ ì„ ì • (ìƒìš© AI ëª¨ë¸ìš©)"""

        # ìƒìš© AI ëª¨ë¸ìš© í• ë‹¹ ì „ëµ
        if k < 50:
            allocation_strategy = {
                'direct_match': 0.35,     # 35% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.05,  # 5% - ê´€ë ¨ ê°œë…
                'differential': 0.0       # 0% - ê¸°íƒ€
            }
        elif k <= 70:
            allocation_strategy = {
                'direct_match': 0.30,     # 30% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.07,  # 7% - ê´€ë ¨ ê°œë…
                'differential': 0.03      # 3% - ê°ë³„ì§„ë‹¨
            }
        else:
            allocation_strategy = {
                'direct_match': 0.25,     # 25% - ì§ì ‘ ë§¤ì¹­
                'prescription': 0.25,     # 25% - ì²˜ë°© ì •ë³´
                'symptom_theory': 0.20,   # 20% - ì¦ìƒ/ì´ë¡ 
                'treatment_method': 0.15,  # 15% - ì¹˜ë£Œë²•
                'related_concept': 0.10,  # 10% - ê´€ë ¨ ê°œë…
                'differential': 0.05      # 5% - ê°ë³„ì§„ë‹¨
            }

        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í›„ë³´ ìˆ˜ í™•ì¸
        total_available = sum(len(cluster_items)
                              for cluster_items in clusters.values())
        actual_k = min(k, total_available)

        if actual_k < k:
            print(f"âš ï¸ ìš”ì²­ëœ Kê°’({k})ë³´ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•œ í›„ë³´ê°€ ì ìŠµë‹ˆë‹¤({total_available}ê°œ)")
            print(f"ğŸ“Š ì‹¤ì œ ì„ ì • ê°€ëŠ¥í•œ ìˆ˜: {actual_k}ê°œ")

        selected = []
        used_content_hashes = set()
        cluster_stats = {}

        # 1ë‹¨ê³„: ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ í• ë‹¹ëœ ë¹„ìœ¨ë§Œí¼ ì„ ì •
        for cluster_name, ratio in allocation_strategy.items():
            cluster_items = clusters.get(cluster_name, [])

            if not cluster_items:
                cluster_stats[cluster_name] = {
                    'target': 0, 'selected': 0, 'available': 0}
                continue

            target_count = max(1, int(actual_k * ratio)) if ratio > 0 else 0

            cluster_selected = 0
            for item in cluster_items:
                if cluster_selected >= target_count or len(selected) >= actual_k:
                    break

                content_hash = hash(item['content'])
                if content_hash in used_content_hashes:
                    continue

                selected.append(item)
                used_content_hashes.add(content_hash)
                cluster_selected += 1

            cluster_stats[cluster_name] = {
                'target': target_count,
                'selected': cluster_selected,
                'available': len(cluster_items)
            }

            if len(selected) >= actual_k:
                break

        # 2ë‹¨ê³„: ë¶€ì¡±í•œ ê²½ìš° ìµœê³  ì ìˆ˜ í•­ëª©ë“¤ë¡œ ì±„ìš°ê¸°
        if len(selected) < actual_k:
            print(f"ğŸ”„ {len(selected)}/{actual_k}ê°œ ì„ ì •ë¨. ì¶”ê°€ í•­ëª©ìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")

            all_remaining = []
            for cluster_items in clusters.values():
                all_remaining.extend(cluster_items)

            all_remaining.sort(key=lambda x: x.get(
                'final_score', x.get('semantic_score', 0)), reverse=True)

            for item in all_remaining:
                if len(selected) >= actual_k:
                    break

                content_hash = hash(item['content'])
                if content_hash not in used_content_hashes:
                    selected.append(item)
                    used_content_hashes.add(content_hash)

        # 3ë‹¨ê³„: ëŒ€ìš©ëŸ‰ì—ì„œ ë‹¤ì–‘ì„± ë³´ì¥ (Kâ‰¥50)
        if actual_k >= 50:
            diversity_enhanced = self._ensure_diversity_for_large_k(
                selected, actual_k)
            selected = diversity_enhanced

        # 4ë‹¨ê³„: ìµœì¢… ì •ë ¬ ë° score í•„ë“œ ì„¤ì •
        for item in selected:
            if 'score' not in item:
                item['score'] = item.get(
                    'final_score', item.get('semantic_score', 0))

        final_selected = sorted(
            selected, key=lambda x: x['score'], reverse=True)[:actual_k]

        # ìƒì„¸ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ì„ ì • ê²°ê³¼:")
        for cluster_name, stats in cluster_stats.items():
            print(f"   {cluster_name}: {stats['selected']}/{stats['target']}ê°œ ì„ ì • "
                  f"(ì‚¬ìš©ê°€ëŠ¥: {stats['available']}ê°œ)")

        print(f"\nğŸ¯ ìµœì¢… ì„ ì •: {len(final_selected)}ê°œ (ìš”ì²­: {k}ê°œ, ì‹¤ì œ: {actual_k}ê°œ)")

        return final_selected

    def _ensure_diversity_for_large_k(self, selected: List[Dict], k: int) -> List[Dict]:
        """ëŒ€ìš©ëŸ‰ Kê°’(50+)ì—ì„œ ë‹¤ì–‘ì„± ë³´ì¥"""

        if k < 50:
            return selected

        # ì¶œì²˜ íŒŒì¼ë³„ ë¶„í¬ ìµœì í™”
        max_per_source = max(5, k // 8)  # ìµœëŒ€ 12.5% ë˜ëŠ” ìµœì†Œ 5ê°œ
        # ëŒ€ë¶„ë¥˜(BB)ë³„ ë¶„í¬ ì œí•œ
        max_per_bb = max(8, k // 6)  # ìµœëŒ€ 16.7% ë˜ëŠ” ìµœì†Œ 8ê°œ

        adjusted_selected = []
        source_counts = {}
        bb_counts = {}

        # 1ì°¨: ì¶œì²˜ì™€ ëŒ€ë¶„ë¥˜ ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì„ ì •
        for item in sorted(selected, key=lambda x: x.get('score', 0), reverse=True):
            source = item['metadata'].get('source_file', 'unknown')
            bb = item['metadata'].get('BB', 'unknown')

            source_count = source_counts.get(source, 0)
            bb_count = bb_counts.get(bb, 0)

            if source_count < max_per_source and bb_count < max_per_bb:
                adjusted_selected.append(item)
                source_counts[source] = source_count + 1
                bb_counts[bb] = bb_count + 1

            if len(adjusted_selected) >= k:
                break

        # 2ì°¨: ë¶€ì¡±í•œ ê²½ìš° ì œí•œì„ ì™„í™”í•˜ì—¬ ì±„ìš°ê¸°
        if len(adjusted_selected) < k:
            remaining = [
                item for item in selected if item not in adjusted_selected]
            needed = k - len(adjusted_selected)

            remaining.sort(key=lambda x: x.get('score', 0), reverse=True)
            adjusted_selected.extend(remaining[:needed])

        # í†µê³„ ì •ë³´ ì¶œë ¥
        final_source_dist = {}
        final_bb_dist = {}
        final_type_dist = {}

        for item in adjusted_selected:
            source = item['metadata'].get('source_file', 'unknown')
            bb = item['metadata'].get('BB', 'unknown')
            content_type = item['metadata'].get('type', 'general')

            final_source_dist[source] = final_source_dist.get(source, 0) + 1
            final_bb_dist[bb] = final_bb_dist.get(bb, 0) + 1
            final_type_dist[content_type] = final_type_dist.get(
                content_type, 0) + 1

        print(
            f"ğŸ“š ì¶œì²˜ ë‹¤ì–‘ì„±: {len(final_source_dist)}ê°œ íŒŒì¼ (ìµœëŒ€ {max_per_source}ê°œ/íŒŒì¼)")
        print(f"ğŸ“– ëŒ€ë¶„ë¥˜ ë‹¤ì–‘ì„±: {len(final_bb_dist)}ê°œ ì˜ì—­ (ìµœëŒ€ {max_per_bb}ê°œ/ì˜ì—­)")
        print(f"ğŸ·ï¸ ë‚´ìš© íƒ€ì…: {final_type_dist}")

        return adjusted_selected
